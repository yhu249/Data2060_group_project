{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5629c8",
   "metadata": {},
   "source": [
    "## Overview of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825cf1d5",
   "metadata": {},
   "source": [
    "Naive Bayes is a family of probabilistic classification methods built on generative modeling, where the goal is to characterize how the features and labels are jointly distributed. Instead of directly learning the conditional probability $P(y \\mid X)$, the method models the joint structure by estimating the class prior distribution together with the class-conditional distributions of the features. The posterior probability used for classification is then obtained via Bayes' rule.\n",
    "\n",
    "At the core of Naive Bayes is the naive conditional independence assumption: given the class label, all features are treated as independent. While this assumption is often violated in real-world data, it greatly simplifies the modeling process, allowing all parameters to be estimated efficiently using simple empirical statistics computed within each class.\n",
    "\n",
    "Different variants of Naive Bayes arise from different assumptions about the class-conditional feature distributions. For example, Multinomial and Bernoulli Naive Bayes are commonly used for text data, while Categorical Naive Bayes applies to discrete, non-numeric features. \n",
    "\n",
    "In this project, we focus on Gaussian Naive Bayes, which assumes that each continuous feature follows a class-conditional normal distribution. This makes GaussianNB particularly suitable for datasets with real-valued features and provides a simple yet effective generative model for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867d0c0",
   "metadata": {},
   "source": [
    "| Variant | Feature Type | Distribution Assumption | Typical Use Case |\n",
    "|--------|--------------|-------------------------|------------------|\n",
    "| **Gaussian Naive Bayes** | Continuous numerical features | Gaussian (normal) distribution | Sensor data, measurements, general continuous features |\n",
    "| **Multinomial Naive Bayes** | Count-based features | Multinomial distribution | Text classification, word counts (BoW, TF) |\n",
    "| **Bernoulli Naive Bayes** | Binary (0/1) features | Bernoulli distribution | Word presence/absence, binary indicators |\n",
    "| **Categorical Naive Bayes** | General categorical features | Categorical distribution | Discrete categorical variables (color, city, etc.) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b95821",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c50e0f",
   "metadata": {},
   "source": [
    "### **Advantages**\n",
    "\n",
    "**1. Extremely fast training**  \n",
    "GaussianNB uses closed-form estimates for class means and variances, requiring no iterative optimization. \n",
    "\n",
    "**2. Performs well on small datasets**  \n",
    "Estimating Gaussian parameters requires relatively few samples, allowing GNB to work effectively even when data is limited.\n",
    "\n",
    "**3. Simple and interpretable**  \n",
    "The contribution of each feature to the final prediction can be clearly understood through class-wise means, variances, and log-likelihoods.\n",
    "\n",
    "**4. Efficient in high-dimensional spaces**  \n",
    "Due to the independence assumption, the computational cost scales linearly with the number of features, making GNB suitable for high-dimensional data.\n",
    "\n",
    "\n",
    "### **Disadvantages**\n",
    "\n",
    "**1. Independence assumption rarely holds**  \n",
    "The naive assumption that features are conditionally independent is often violated in real-world data, which can degrade model performance.\n",
    "\n",
    "**2. Sensitive to distribution mismatch**  \n",
    "GNB assumes each feature follows a Gaussian distribution within each class. Strongly skewed, multimodal, or heavy-tailed distributions may lead to poor results.\n",
    "\n",
    "**3. Limited model flexibility**  \n",
    "The decision boundaries are quadratic, restricting the model’s ability to capture more complex nonlinear structures.\n",
    "\n",
    "**4. Assumes each feature dimension is an independent univariate Gaussian**  \n",
    "GNB does not capture correlations between features (i.e., it does not model full multivariate Gaussian distributions), which can be limiting when feature interactions are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd5f77",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c326dc",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes is a generative model that assumes the data are generated from class-conditional Gaussian distributions with a simple independence structure.\n",
    "\n",
    "**Class prior:**\n",
    "\n",
    "For each class c, the model includes a class prior\n",
    "\n",
    "$$\n",
    "\\pi_c = P(y = c),\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\sum_c \\pi_c = 1.\n",
    "$$\n",
    "\n",
    "These priors describe how frequent each class is in the population.\n",
    "\n",
    "**Class-conditional feature distributions:**\n",
    "\n",
    "Conditioned on the class label y = c, each feature $x_j$ is assumed to be generated independently from a univariate Gaussian:\n",
    "\n",
    "$$\n",
    "x_j \\mid y = c \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2),\n",
    "$$\n",
    "\n",
    "where $\\mu_{c,j}$ and $sigma_{c,j}^2$ are unknown population parameters: the true mean and variance of feature j within class c.\n",
    "\n",
    "Under the Naive Bayes conditional independence assumption, the class-conditional density factorizes as\n",
    "\n",
    "$$\n",
    "P(x \\mid y = c)\n",
    "= \\prod_{j=1}^d\n",
    "\\mathcal{N}\\!\\big(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2\\big).\n",
    "$$\n",
    "\n",
    "**Joint model and parameter set:**\n",
    "\n",
    "The joint distribution over features and labels is\n",
    "\n",
    "$$\n",
    "P(x, y = c)\n",
    "= \\pi_c \\prod_{j=1}^d\n",
    "\\mathcal{N}\\!\\big(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2\\big).\n",
    "$$\n",
    "\n",
    "The full parameter set that defines the model is\n",
    "\n",
    "$$\n",
    "\\Theta\n",
    "= \\{\\pi_c, \\mu_{c,j}, \\sigma_{c,j}^2\\}_{c=1,\\dots,K;\\; j=1,\\dots,d},\n",
    "$$\n",
    "\n",
    "i.e., one prior $\\pi_c$ per class and one Gaussian mean/variance pair $\\mu_{c,j}$, $\\sigma_{c,j}^2$ for each feature within each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b2a3b",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca128f",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes parameters are estimated by maximizing the likelihood of the training data, or equivalently, minimizing the negative log-likelihood (NLL).  \n",
    "Given training samples $\\{(x_i, y_i)\\}_{i=1}^n$ with $x_i = (x_{i,1}, \\dots, x_{i,d})$, the joint model is\n",
    "\n",
    "$$\n",
    "P(x_i, y_i)\n",
    "= \\pi_{y_i} \\prod_{j=1}^d \n",
    "\\mathcal{N}(x_{i,j} \\mid \\mu_{y_i,j}, \\sigma_{y_i,j}^2).\n",
    "$$\n",
    "\n",
    "Taking the negative log of the likelihood over all samples yields the loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\Theta)\n",
    "=\n",
    "\\sum_{i=1}^n \n",
    "\\left[\n",
    "-\\log \\pi_{y_i}\n",
    "+\n",
    "\\sum_{j=1}^d\n",
    "\\left(\n",
    "\\frac{1}{2}\\log(2\\pi \\sigma_{y_i,j}^2)\n",
    "+\n",
    "\\frac{(x_{i,j} - \\mu_{y_i,j})^2}{2\\sigma_{y_i,j}^2}\n",
    "\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "This NLL loss arises directly from the Gaussian pdf and the Naive Bayes independence assumption.  \n",
    "Although this is the formal loss minimized by the model, its optimal parameters have closed-form solutions, so no iterative optimizer is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f37987a",
   "metadata": {},
   "source": [
    "## Optimizer(Not Real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba7237",
   "metadata": {},
   "source": [
    "Unlike discriminative models such as logistic regression or neural networks,  \n",
    "Gaussian Naive Bayes does not require an iterative optimizer.  \n",
    "Although the model minimizes the negative log-likelihood (NLL), the optimal parameters\n",
    "have closed-form maximum likelihood solutions.\n",
    "\n",
    "For each class $c$ and feature $j$, the MLE updates are:\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_c = \\frac{N_c}{N},\n",
    "$$\n",
    "where $N_c$ is the number of samples in class $c$.\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{c,j}\n",
    "= \\frac{1}{N_c} \\sum_{i : y_i = c} x_{i,j}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{c,j}^2\n",
    "= \\frac{1}{N_c} \\sum_{i : y_i = c} (x_{i,j} - \\hat{\\mu}_{c,j})^2.\n",
    "$$\n",
    "\n",
    "These formulas directly minimize the NLL loss, so training requires only computing\n",
    "class counts, sample means, and sample variances—no gradient descent, no iterative\n",
    "optimization, and no numerical solver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d727c00",
   "metadata": {},
   "source": [
    "## Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57cdc33",
   "metadata": {},
   "source": [
    "Training Gaussian Naive Bayes model  \n",
    "**Require:** Training dataset $(X, y)$  \n",
    "**Ensure:** Class priors $P(c)$, means $\\mu_{c,j}$, variances $\\sigma_{c,j}^2$  \n",
    "\n",
    "1:  $C \\leftarrow \\text{unique}(y)$  \n",
    "2:  **for each** class $c \\in C$ **do**  \n",
    "3:  $S_c \\leftarrow \\{\\, i : y_i = c \\,\\}$  \n",
    "4:  $N_c \\leftarrow |S_c|$  \n",
    "5:  $P(c) \\leftarrow \\frac{N_c}{N}$  \n",
    "6:  **for** $j = 1 \\dots d$ **do**  \n",
    "7:   $X_{c,j} \\leftarrow \\{\\, X_{i,j} : i \\in S_c \\,\\}$  \n",
    "8:   $\\mu_{c,j} \\leftarrow \\frac{1}{N_c} \\sum_{i \\in S_c} X_{i,j}$  \n",
    "9:   $\\sigma_{c,j}^2 \\leftarrow \\frac{1}{N_c} \\sum_{i \\in S_c} (X_{i,j} - \\mu_{c,j})^2 + \\epsilon$  \n",
    "10:  **end for**  \n",
    "11: **end for**  \n",
    "\n",
    "---\n",
    "\n",
    "Predicting with Gaussian Naive Bayes  \n",
    "**Require:** Model parameters $P(c)$, $\\mu_{c,j}$, $\\sigma_{c,j}^2$  \n",
    "**Ensure:** Predicted label $\\hat{y}$  \n",
    "\n",
    "1:  **for each** class $c \\in C$ **do**  \n",
    "2:  $\\text{score}(c) \\leftarrow \\log P(c)$  \n",
    "3:  **for** $j = 1 \\dots d$ **do**  \n",
    "4:   $\\ell \\leftarrow -\\frac{1}{2}\\log(2\\pi\\sigma_{c,j}^2) \n",
    "      \\;-\\; \\frac{(x_j - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}$  \n",
    "5:   $\\text{score}(c) \\leftarrow \\text{score}(c) + \\ell$  \n",
    "6:  **end for**  \n",
    "7: **end for**  \n",
    "8:  $\\hat{y} \\leftarrow \\arg\\max_{c} \\text{score}(c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fefeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
