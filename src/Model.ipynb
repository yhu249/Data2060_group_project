{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ca05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    A Naive Bayes classifier that models all features as Gaussian\n",
    "    (continuous numerical values following a normal distribution).\n",
    "\n",
    "    All probability calculations are done in log space to avoid numerical\n",
    "    underflow when probabilities become very small. During prediction the model\n",
    "    converts log probabilities back into normal probabilities and normalizes\n",
    "    them so that each row sums to one.\n",
    "\n",
    "    Feature likelihoods\n",
    "    -------------------\n",
    "    For Gaussian numeric features:\n",
    "        log_pdf = -0.5 * ( log(2 * pi * variance) + ((x - mean)^2) / variance )\n",
    "\n",
    "    The model uses maximum likelihood estimates of the mean and variance\n",
    "    for each feature within each class, with an additional variance smoothing\n",
    "    term added for numerical stability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eps : float\n",
    "        Small constant added to Gaussian variances for stability.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This version of the classifier supports only continuous numerical features.\n",
    "    Any preprocessing or transformations needed to produce numeric inputs\n",
    "    must be done before calling fit.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> X = np.array([[5.1, 3.5],\n",
    "                      [4.9, 3.0],\n",
    "                      [5.0, 3.4]])\n",
    "    >>> y = np.array([\"A\", \"B\", \"A\"])\n",
    "    >>> model = MixedNaiveBayes()\n",
    "    >>> model.fit(X, y)\n",
    "    >>> model.predict(X)\n",
    "    array(['A', 'B', 'A'], dtype='<U1')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-9):\n",
    "        self.eps = eps\n",
    "\n",
    "        self.feature_types = None\n",
    "        self.classes_ = None\n",
    "\n",
    "        self.log_priors_ = {}\n",
    "\n",
    "        self.gaussian_means_ = {}\n",
    "        self.gaussian_vars_ = {}\n",
    "        self.bernoulli_probs_ = {}\n",
    "        self.categorical_probs_ = {}\n",
    "        self.categorical_values_ = {}\n",
    "\n",
    "    # Likelihood helper functions\n",
    "    def _gaussian_log_pdf(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Computes the log density of a Gaussian distribution for a single feature.\n",
    "        This function is for continuous values.\n",
    "        \"\"\"\n",
    "        return -0.5 * (np.log(2.0 * np.pi * var) + ((x - mean) ** 2) / var)\n",
    "\n",
    "    # Aggregator\n",
    "    def _compute_log_likelihood(self, x_row, class_label):\n",
    "        \"\"\"\n",
    "        Computes the total log likelihood of a sample under a specific class.\n",
    "        This is the sum of the log likelihoods of each feature.\n",
    "        \"\"\"\n",
    "        log_l = 0.0\n",
    "\n",
    "        for j in range(len(x_row)):\n",
    "            mean = self.gaussian_means_[class_label][j]\n",
    "            var  = self.gaussian_vars_[class_label][j]\n",
    "            log_l += self._gaussian_log_pdf(x_row[j], mean, var)\n",
    "\n",
    "        return log_l\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Learns Gaussian Naive Bayes parameters: class priors,\n",
    "        per-class means and variances (with sklearn-style smoothing).\n",
    "\n",
    "        Inputs\n",
    "        ------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data (numeric only).\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Class labels.\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        Returns the trained model instance.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        # global variance for smoothing\n",
    "        global_var = X.var()\n",
    "\n",
    "        # class priors\n",
    "        for c in self.classes_:\n",
    "            n_c = np.sum(y == c)\n",
    "            self.log_priors_[c] = np.log(n_c / n_samples)\n",
    "\n",
    "        # init Gaussian dicts\n",
    "        for c in self.classes_:\n",
    "            self.gaussian_means_[c] = {}\n",
    "            self.gaussian_vars_[c] = {}\n",
    "\n",
    "        # compute Gaussian parameters\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y == c]\n",
    "\n",
    "            means = Xc.mean(axis=0)\n",
    "            vars_  = Xc.var(axis=0)\n",
    "\n",
    "            # smoothing:\n",
    "            # var += eps * global_variance\n",
    "            smoothed_var = vars_ + self.eps * global_var\n",
    "\n",
    "            for j in range(n_features):\n",
    "                self.gaussian_means_[c][j] = means[j]\n",
    "                self.gaussian_vars_[c][j] = smoothed_var[j]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Prediction\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Computes unnormalized log posterior scores for each class.\n",
    "        These are the raw log scores before normalization.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, len(self.classes_)))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_row = X[i]\n",
    "            for k, c in enumerate(self.classes_):\n",
    "                log_l = self._compute_log_likelihood(x_row, c)\n",
    "                log_probs[i, k] = self.log_priors_[c] + log_l\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Converts log posterior scores into normalized probabilities.\n",
    "        Uses the log-sum-exp trick for numerical stability.\n",
    "        \"\"\"\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "\n",
    "        max_log = np.max(log_probs, axis=1, keepdims=True)\n",
    "        shifted = log_probs - max_log\n",
    "\n",
    "        probs = np.exp(shifted)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Returns the predicted class label for each sample.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        class_indices = np.argmax(probs, axis=1)\n",
    "        return self.classes_[class_indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
