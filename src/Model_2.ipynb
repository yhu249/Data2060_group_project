{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ca05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier.\n",
    "\n",
    "    \n",
    "    This classifier assumes all features follow a Gaussian distribution and\n",
    "    estimates per-class means and variances. Variance smoothing is applied\n",
    "    using the formula: epsilon = var_smoothing * max(feature_variances)\n",
    "\n",
    "    All probability calculations are done in log space to avoid numerical\n",
    "    underflow when probabilities become very small. During prediction the model\n",
    "    converts log probabilities back into normal probabilities and normalizes\n",
    "    them so that each row sums to one.\n",
    "\n",
    "    Feature likelihoods\n",
    "    ----------\n",
    "    For Gaussian numeric features:\n",
    "        log_pdf = -0.5 * ( log(2 * pi * variance) + ((x - mean)^2) / variance )\n",
    "\n",
    "    The model uses maximum likelihood estimates of the mean and variance\n",
    "    for each feature within each class, with an additional variance smoothing\n",
    "    term added for numerical stability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var_smoothing : float, optional (default=1e-9)\n",
    "        Portion of the largest feature variance added to each variance estimate\n",
    "        for numerical stability.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> X = np.array([[1.0, 2.0],\n",
    "    ...               [1.2, 1.9],\n",
    "    ...               [3.2, 4.8],\n",
    "    ...               [3.0, 5.1]])\n",
    "    >>> y = np.array(['A', 'A', 'B', 'B'])\n",
    "    >>> model = GaussianNaiveBayes()\n",
    "    >>> model.fit(X, y)\n",
    "    >>> model.predict([[1.1, 2.0]])\n",
    "    array(['A'], dtype='<U1')\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.log_priors_ = {}\n",
    "        self.gaussian_means_ = {}\n",
    "        self.gaussian_vars_ = {}\n",
    "\n",
    "    # Likelihood helper functions\n",
    "    def _gaussian_log_pdf(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Compute the log-density of a Gaussian distribution for a single feature.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float\n",
    "            Observed value of the feature.\n",
    "        mean : float\n",
    "            Mean of the feature for the given class.\n",
    "        var : float\n",
    "            Variance of the feature for the given class (after smoothing).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The log probability density log(P(x | mean, var)).\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model._gaussian_log_pdf(2.0, mean=2.0, var=1.0)\n",
    "        0.0\n",
    "        \"\"\"\n",
    "        \n",
    "        return -0.5 * (np.log(2.0 * np.pi * var) + ((x - mean) ** 2) / var)\n",
    "\n",
    "    # Aggregator\n",
    "    def _compute_log_likelihood(self, x_row, class_label):\n",
    "        \"\"\"\n",
    "        Compute the total log-likelihood log p(x | c) of a single sample under\n",
    "        a given class using the Gaussian Naive Bayes assumption.\n",
    "\n",
    "        Under the Naive Bayes conditional independence (i.i.d.) assumption,\n",
    "        the joint likelihood factorizes across features:\n",
    "            p(x | c) = ∏_j p(x_j | c)\n",
    "            log p(x | c) = ∑_j log p(x_j | c)\n",
    "\n",
    "        Each feature likelihood p(x_j | c) is modeled as a univariate Gaussian\n",
    "        distribution with class-specific mean and variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_row : array-like of shape (n_features,)\n",
    "            A single data point.\n",
    "        class_label : object\n",
    "            The class for which likelihood is being computed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The total log-likelihood log p(x | c), computed as the sum of \n",
    "            Gaussian log-densities across all features.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "            - This method operates fully in log space to ensure numerical stability.\n",
    "            - The method assumes that Gaussian means and variances for each class\n",
    "            have already been estimated and stored in:\n",
    "                - self.gaussian_means_[class_label]\n",
    "                - self.gaussian_vars_[class_label]\n",
    "\n",
    "            Algorithm\n",
    "            ---------\n",
    "            1. Initialize log-likelihood accumulator log_l = 0\n",
    "            2. For each feature j:\n",
    "                a. Retrieve class-specific mean μ_{c,j} and variance σ²_{c,j}\n",
    "                b. Compute log p(x_j | c) using a Gaussian log-pdf\n",
    "                c. Add the result to log_l\n",
    "            3. Return the accumulated log-likelihood\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0}}\n",
    "        >>> model.gaussian_vars_ = {'A': {0: 1.0}}\n",
    "        >>> model._compute_log_likelihood([0.0], 'A')\n",
    "        0.0\n",
    "\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0, 1: 1.0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1.0, 1: 1.0}}\n",
    "        >>> model._compute_log_likelihood([0.0, 1.0], 'A')\n",
    "        -1.8378770664093453\n",
    "        \"\"\"\n",
    "        log_l = 0.0\n",
    "\n",
    "        for j in range(len(x_row)):\n",
    "            mean = self.gaussian_means_[class_label][j]\n",
    "            var  = self.gaussian_vars_[class_label][j]\n",
    "            log_l += self._gaussian_log_pdf(x_row[j], mean, var)\n",
    "\n",
    "        return log_l\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Naive Bayes classifier.\n",
    "\n",
    "        Learns:\n",
    "        - class prior probabilities log(P(Y=c))\n",
    "        - per-class feature means\n",
    "        - per-class feature variances (with sklearn-style smoothing)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data matrix of continuous features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Class labels for each sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : GaussianNaiveBayes\n",
    "            The fitted model instance.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> X = np.array([[1, 2], [1.1, 1.8], [3, 4.9]])\n",
    "        >>> y = np.array(['A', 'A', 'B'])\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.fit(X, y)\n",
    "        GaussianNaiveBayes(...)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        if np.isnan(X).any():\n",
    "            raise ValueError(\"GaussianNaiveBayes does not support missing values. \"\n",
    "                     \"Please remove or impute missing data before fitting.\")\n",
    "\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        feature_vars = X.var(axis=0)          # per-feature variance\n",
    "        max_var = np.max(feature_vars)        # biggest variance across features\n",
    "        epsilon = self.var_smoothing * max_var \n",
    "\n",
    "        # class priors\n",
    "        for c in self.classes_:\n",
    "            n_c = np.sum(y == c)\n",
    "            self.log_priors_[c] = np.log(n_c / n_samples)\n",
    "\n",
    "        # init Gaussian dicts\n",
    "        for c in self.classes_:\n",
    "            self.gaussian_means_[c] = {}\n",
    "            self.gaussian_vars_[c] = {}\n",
    "\n",
    "        # compute Gaussian parameters\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y == c]\n",
    "\n",
    "            means = Xc.mean(axis=0)\n",
    "            vars_  = Xc.var(axis=0)\n",
    "\n",
    "            # smoothing:\n",
    "            # var += eps * global_variance\n",
    "            smoothed_vars = vars_ + epsilon\n",
    "\n",
    "            for j in range(n_features):\n",
    "                self.gaussian_means_[c][j] = means[j]\n",
    "                self.gaussian_vars_[c][j] = smoothed_vars[j]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Prediction\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute unnormalized log posterior scores log P(Y = c | X) for each sample\n",
    "        and each class using Gaussian Naive Bayes.\n",
    "\n",
    "        Using Bayes' rule, the posterior is proportional to the product of the\n",
    "        class prior and the class-conditional likelihood:\n",
    "            P(c | x) ∝ P(c) · P(x | c)\n",
    "\n",
    "        Taking the logarithm gives:\n",
    "            log P(c | x) = log P(c) + log P(x | c)\n",
    "\n",
    "        where log P(x | c) is computed under the Naive Bayes conditional\n",
    "        independence (i.i.d.) assumption as the sum of per-feature Gaussian\n",
    "        log-likelihoods.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples, where each row corresponds to a single data point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Log posterior scores (not normalized).\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        - All computations are performed in log space for numerical stability.\n",
    "        - This method assumes that the following attributes have already been\n",
    "        estimated during fitting:\n",
    "            - self.classes_\n",
    "            - self.log_priors_\n",
    "            - self.gaussian_means_\n",
    "            - self.gaussian_vars_\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Initialize a matrix log_probs of shape (n_samples, n_classes)\n",
    "        2. For each sample i:\n",
    "            a. Extract the sample x_row = X[i]\n",
    "            b. For each class c:\n",
    "                i.   Compute log-likelihood log P(x | c) using _compute_log_likelihood\n",
    "                ii.  Add the class log prior log P(c)\n",
    "                iii. Store the result in log_probs[i, c]\n",
    "        3. Return the log_probs matrix            \n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1.0}}\n",
    "        >>> model.predict_log_proba([[0]])\n",
    "        array([[0.]])\n",
    "\n",
    "        >>> model.classes_ = np.array(['A', 'B'])\n",
    "        >>> model.log_priors_ = {'A': np.log(0.5), 'B': np.log(0.5)}\n",
    "        >>> model.gaussian_means_ = {\n",
    "        ...     'A': {0: 0.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.gaussian_vars_ = {\n",
    "        ...     'A': {0: 1.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.predict_log_proba([[0.0], [1.0]])\n",
    "        array([[-0.9189..., -1.4189...],\n",
    "            [-1.4189..., -0.9189...]])\n",
    "            \n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, len(self.classes_)))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_row = X[i]\n",
    "            for k, c in enumerate(self.classes_):\n",
    "                log_l = self._compute_log_likelihood(x_row, c)\n",
    "                log_probs[i, k] = self.log_priors_[c] + log_l\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute normalized posterior probabilities P(Y = c | X) for each sample\n",
    "        using the Gaussian Naive Bayes model.\n",
    "\n",
    "        This method converts unnormalized log posterior scores obtained from\n",
    "        `predict_log_proba` into valid probability distributions by applying\n",
    "        the softmax function. To ensure numerical stability, the log-sum-exp\n",
    "        trick is used by shifting log probabilities before exponentiation.\n",
    "\n",
    "        Specifically, for each sample x:\n",
    "            P(c | x) = exp(log P(c | x)) / sum_k exp(log P(k | x))\n",
    "        where log P(c | x) = log P(c) + log P(x | c).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Normalized probabilities. Each row sums to 1.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - This method relies on `predict_log_proba` to compute unnormalized\n",
    "        log posterior scores.\n",
    "        - For numerical stability, the maximum log posterior value is subtracted\n",
    "        from each row before exponentiation:\n",
    "            shifted = log_probs - max(log_probs)\n",
    "\n",
    "        This does not change the resulting probabilities because softmax\n",
    "        is invariant to constant shifts.\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Compute unnormalized log posterior scores using predict_log_proba(X)\n",
    "        2. For each sample, subtract the maximum log score (numerical stability)\n",
    "        3. Exponentiate the shifted log scores\n",
    "        4. Normalize by dividing by the row-wise sum\n",
    "        5. Return normalized probabilities\n",
    "\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1}}\n",
    "        >>> model.predict_proba([[0]])\n",
    "        array([[1.]])\n",
    "\n",
    "        >>> model.classes_ = np.array(['A', 'B'])\n",
    "        >>> model.log_priors_ = {'A': np.log(0.5), 'B': np.log(0.5)}\n",
    "        >>> model.gaussian_means_ = {\n",
    "        ...     'A': {0: 0.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.gaussian_vars_ = {\n",
    "        ...     'A': {0: 1.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.predict_proba([[0.0], [1.0]])\n",
    "        array([[0.6224..., 0.3775...],\n",
    "            [0.3775..., 0.6224...]])\n",
    "        \"\"\"\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        max_log = np.max(log_probs, axis=1, keepdims=True)\n",
    "        shifted = log_probs - max_log\n",
    "\n",
    "        probs = np.exp(shifted)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the most likely class label for each sample.\n",
    "\n",
    "        This method first computes normalized posterior probabilities using\n",
    "        `predict_proba`, and then selects the class with the highest probability\n",
    "        for each sample. Formally, the predicted class is:\n",
    "            y_hat = argmax_c P(Y = c | x)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "            Predicted class labels corresponding to the maximum posterior\n",
    "            probability for each sample.\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Compute normalized posterior probabilities using predict_proba(X)\n",
    "        2. Find the index of the maximum probability for each sample\n",
    "        3. Map indices to class labels using self.classes_\n",
    "        4. Return predicted class labels\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> X = np.array([[0], [2]])\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1}}\n",
    "        >>> model.predict(X)\n",
    "        array(['A', 'A'], dtype='<U1')\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        class_indices = np.argmax(probs, axis=1)\n",
    "        return self.classes_[class_indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
