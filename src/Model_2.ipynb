{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ca05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier.\n",
    "\n",
    "    \n",
    "    This classifier assumes all features follow a Gaussian distribution and\n",
    "    estimates per-class means and variances. Variance smoothing is applied\n",
    "    using the formula: epsilon = var_smoothing * max(feature_variances)\n",
    "\n",
    "    All probability calculations are done in log space to avoid numerical\n",
    "    underflow when probabilities become very small. During prediction the model\n",
    "    converts log probabilities back into normal probabilities and normalizes\n",
    "    them so that each row sums to one.\n",
    "\n",
    "    Feature likelihoods\n",
    "    ----------\n",
    "    For Gaussian numeric features:\n",
    "        log_pdf = -0.5 * ( log(2 * pi * variance) + ((x - mean)^2) / variance )\n",
    "\n",
    "    The model uses maximum likelihood estimates of the mean and variance\n",
    "    for each feature within each class, with an additional variance smoothing\n",
    "    term added for numerical stability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var_smoothing : float, optional (default=1e-9)\n",
    "        Portion of the largest feature variance added to each variance estimate\n",
    "        for numerical stability.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> X = np.array([[1.0, 2.0],\n",
    "    ...               [1.2, 1.9],\n",
    "    ...               [3.2, 4.8],\n",
    "    ...               [3.0, 5.1]])\n",
    "    >>> y = np.array(['A', 'A', 'B', 'B'])\n",
    "    >>> model = GaussianNaiveBayes()\n",
    "    >>> model.fit(X, y)\n",
    "    >>> model.predict([[1.1, 2.0]])\n",
    "    array(['A'], dtype='<U1')\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.log_priors_ = {}\n",
    "        self.gaussian_means_ = {}\n",
    "        self.gaussian_vars_ = {}\n",
    "\n",
    "    # Likelihood helper functions\n",
    "    def _gaussian_log_pdf(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Compute the log-density of a Gaussian distribution for a single feature.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float\n",
    "            Observed value of the feature.\n",
    "        mean : float\n",
    "            Mean of the feature for the given class.\n",
    "        var : float\n",
    "            Variance of the feature for the given class (after smoothing).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The log probability density log(P(x | mean, var)).\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model._gaussian_log_pdf(2.0, mean=2.0, var=1.0)\n",
    "        0.0\n",
    "        \"\"\"\n",
    "        \n",
    "        return -0.5 * (np.log(2.0 * np.pi * var) + ((x - mean) ** 2) / var)\n",
    "\n",
    "    # Aggregator\n",
    "    def _compute_log_likelihood(self, x_row, class_label):\n",
    "        \"\"\"\n",
    "        Compute the total log-likelihood of a sample under a given class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_row : array-like of shape (n_features,)\n",
    "            A single data point.\n",
    "        class_label : object\n",
    "            The class for which likelihood is being computed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Sum of log-likelihoods across all features.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0}}\n",
    "        >>> model.gaussian_vars_ = {'A': {0: 1.0}}\n",
    "        >>> model._compute_log_likelihood([0.0], 'A')\n",
    "        0.0\n",
    "        \"\"\"\n",
    "        log_l = 0.0\n",
    "\n",
    "        for j in range(len(x_row)):\n",
    "            mean = self.gaussian_means_[class_label][j]\n",
    "            var  = self.gaussian_vars_[class_label][j]\n",
    "            log_l += self._gaussian_log_pdf(x_row[j], mean, var)\n",
    "\n",
    "        return log_l\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Naive Bayes classifier.\n",
    "\n",
    "        Learns:\n",
    "        - class prior probabilities log(P(Y=c))\n",
    "        - per-class feature means\n",
    "        - per-class feature variances (with sklearn-style smoothing)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data matrix of continuous features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Class labels for each sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : GaussianNaiveBayes\n",
    "            The fitted model instance.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> X = np.array([[1, 2], [1.1, 1.8], [3, 4.9]])\n",
    "        >>> y = np.array(['A', 'A', 'B'])\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.fit(X, y)\n",
    "        GaussianNaiveBayes(...)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y)\n",
    "\n",
<<<<<<< HEAD
    "        if np.isnan(X).any():\n",
    "            raise ValueError(\"GaussianNaiveBayes does not support missing values. \"\n",
    "                     \"Please remove or impute missing data before fitting.\")\n",
    "\n",
    "\n",
=======
>>>>>>> f8717f0e1dedd2319da43d3c8be756e555914b19
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        feature_vars = X.var(axis=0)          # per-feature variance\n",
    "        max_var = np.max(feature_vars)        # biggest variance across features\n",
    "        epsilon = self.var_smoothing * max_var \n",
    "\n",
    "        # class priors\n",
    "        for c in self.classes_:\n",
    "            n_c = np.sum(y == c)\n",
    "            self.log_priors_[c] = np.log(n_c / n_samples)\n",
    "\n",
    "        # init Gaussian dicts\n",
    "        for c in self.classes_:\n",
    "            self.gaussian_means_[c] = {}\n",
    "            self.gaussian_vars_[c] = {}\n",
    "\n",
    "        # compute Gaussian parameters\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y == c]\n",
    "\n",
    "            means = Xc.mean(axis=0)\n",
    "            vars_  = Xc.var(axis=0)\n",
    "\n",
    "            # smoothing:\n",
    "            # var += eps * global_variance\n",
    "            smoothed_vars = vars_ + epsilon\n",
    "\n",
    "            for j in range(n_features):\n",
    "                self.gaussian_means_[c][j] = means[j]\n",
    "                self.gaussian_vars_[c][j] = smoothed_vars[j]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Prediction\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute unnormalized log posterior scores log(P(Y=c | X)).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Log posterior scores (not normalized).\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1.0}}\n",
    "        >>> model.predict_log_proba([[0]])\n",
    "        array([[0.]])\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, len(self.classes_)))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_row = X[i]\n",
    "            for k, c in enumerate(self.classes_):\n",
    "                log_l = self._compute_log_likelihood(x_row, c)\n",
    "                log_probs[i, k] = self.log_priors_[c] + log_l\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute normalized posterior probabilities P(Y=c | X).\n",
    "\n",
    "        Uses the log-sum-exp trick to avoid numerical underflow.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Normalized probabilities.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1}}\n",
    "        >>> model.predict_proba([[0]])\n",
    "        array([[1.]])\n",
    "        \"\"\"\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        max_log = np.max(log_probs, axis=1, keepdims=True)\n",
    "        shifted = log_probs - max_log\n",
    "\n",
    "        probs = np.exp(shifted)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the most likely class for each sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> X = np.array([[0], [2]])\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1}}\n",
    "        >>> model.predict(X)\n",
    "        array(['A', 'A'], dtype='<U1')\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        class_indices = np.argmax(probs, axis=1)\n",
    "        return self.classes_[class_indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
