{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463e43e9",
   "metadata": {},
   "source": [
    "## Overview of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ccc83",
   "metadata": {},
   "source": [
    "Naive Bayes is a family of probabilistic classification methods built on generative modeling, where the goal is to characterize how the features and labels are jointly distributed. Instead of directly learning the conditional probability $P(y \\mid X)$, the method models the joint structure by estimating the class prior distribution together with the class-conditional distributions of the features. The posterior probability used for classification is then obtained via Bayes' rule.\n",
    "\n",
    "At the core of Naive Bayes is the naive conditional independence assumption: given the class label, all features are treated as independent. While this assumption is often violated in real-world data, it greatly simplifies the modeling process, allowing all parameters to be estimated efficiently using simple empirical statistics computed within each class.\n",
    "\n",
    "Different variants of Naive Bayes arise from different assumptions about the class-conditional feature distributions. For example, Multinomial and Bernoulli Naive Bayes are commonly used for text data, while Categorical Naive Bayes applies to discrete, non-numeric features. \n",
    "\n",
    "In this project, we focus on Gaussian Naive Bayes, which assumes that each continuous feature follows a class-conditional normal distribution. This makes GaussianNB particularly suitable for datasets with real-valued features and provides a simple yet effective generative model for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3c857",
   "metadata": {},
   "source": [
    "| Variant | Feature Type | Distribution Assumption | Typical Use Case |\n",
    "|--------|--------------|-------------------------|------------------|\n",
    "| **Gaussian Naive Bayes** | Continuous numerical features | Gaussian (normal) distribution | Sensor data, measurements, general continuous features |\n",
    "| **Multinomial Naive Bayes** | Count-based features | Multinomial distribution | Text classification, word counts (BoW, TF) |\n",
    "| **Bernoulli Naive Bayes** | Binary (0/1) features | Bernoulli distribution | Word presence/absence, binary indicators |\n",
    "| **Categorical Naive Bayes** | General categorical features | Categorical distribution | Discrete categorical variables (color, city, etc.) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdf00a",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616631f1",
   "metadata": {},
   "source": [
    "### **Advantages**\n",
    "\n",
    "**1. Extremely fast training**  \n",
    "GaussianNB uses closed-form estimates for class means and variances, requiring no iterative optimization. \n",
    "\n",
    "**2. Performs well on small datasets**  \n",
    "Estimating Gaussian parameters requires relatively few samples, allowing GNB to work effectively even when data is limited.\n",
    "\n",
    "**3. Simple and interpretable**  \n",
    "The contribution of each feature to the final prediction can be clearly understood through class-wise means, variances, and log-likelihoods.\n",
    "\n",
    "**4. Efficient in high-dimensional spaces**  \n",
    "Due to the independence assumption, the computational cost scales linearly with the number of features, making GNB suitable for high-dimensional data.\n",
    "\n",
    "\n",
    "### **Disadvantages**\n",
    "\n",
    "**1. Independence assumption rarely holds**  \n",
    "The naive assumption that features are conditionally independent is often violated in real-world data, which can degrade model performance.\n",
    "\n",
    "**2. Sensitive to distribution mismatch**  \n",
    "GNB assumes each feature follows a Gaussian distribution within each class. Strongly skewed, multimodal, or heavy-tailed distributions may lead to poor results.\n",
    "\n",
    "**3. Limited model flexibility**  \n",
    "The decision boundaries are quadratic, restricting the model’s ability to capture more complex nonlinear structures.\n",
    "\n",
    "**4. Assumes each feature dimension is an independent univariate Gaussian**  \n",
    "GNB does not capture correlations between features (i.e., it does not model full multivariate Gaussian distributions), which can be limiting when feature interactions are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c64f7",
   "metadata": {},
   "source": [
    "## Problem Assumptions\n",
    "\n",
    "Gaussian Naive Bayes is a **generative classifier**:  \n",
    "it models the joint distribution \\(p(x, y)\\) and then uses Bayes’ rule\n",
    "to predict the most likely class \\(y\\) for a new feature vector \\(x\\).\n",
    "\n",
    "---\n",
    "\n",
    "### Notation\n",
    "\n",
    "- \\(x \\in \\mathbb{R}^d\\): feature vector with \\(d\\) continuous attributes  \n",
    "- \\(y \\in \\{c_1, \\dots, c_K\\}\\): class label (categorical)  \n",
    "- \\(\\mu_{c,j}, \\sigma_{c,j}^2\\): mean and variance of feature \\(j\\) in class \\(c\\)\n",
    "\n",
    "---\n",
    "\n",
    "### Assumption 1 — Class-conditional Gaussian distributions\n",
    "\n",
    "For each class \\(c\\) and each feature \\(j\\), the feature value follows\n",
    "a **Gaussian distribution** within that class:\n",
    "\n",
    "$$\n",
    "x_j \\mid (y = c) \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2).\n",
    "$$\n",
    "\n",
    "So, conditioned on the class, each feature is modeled\n",
    "as a one-dimensional normal variable with its own mean and variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumption 2 — Naive conditional independence & generative model\n",
    "\n",
    "Given the class label \\(y = c\\), all features are assumed to be\n",
    "**conditionally independent**:\n",
    "\n",
    "$$\n",
    "p(x \\mid y = c)\n",
    "= \\prod_{j=1}^d p(x_j \\mid y = c).\n",
    "$$\n",
    "\n",
    "Using this together with the class prior \\(p(y = c)\\),\n",
    "the **generative joint model** factorizes as\n",
    "\n",
    "$$\n",
    "p(x, y = c)\n",
    "= p(y = c)\\, p(x \\mid y = c)\n",
    "= \\pi_c \\prod_{j=1}^d \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2),\n",
    "$$\n",
    "\n",
    "where \\(\\pi_c = p(y = c)\\) is the class prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1563ed",
   "metadata": {},
   "source": [
    "## Representation\n",
    "\n",
    "Gaussian Naive Bayes is a **generative classifier**.  \n",
    "It models how the pair $(x, y)$ is generated, and then uses Bayes’ rule to\n",
    "predict the most likely class for a new feature vector $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Bayes rule\n",
    "\n",
    "We start from Bayes’ rule for a single class $c$:\n",
    "\n",
    "$$\n",
    "P(y = c \\mid x)\n",
    "= \\frac{P(x \\mid y = c)\\, P(y = c)}{P(x)} .\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $P(y = c \\mid x)$: posterior — probability that the label is class $c$ given $x$  \n",
    "- $P(x \\mid y = c)$: likelihood of seeing features $x$ under class $c$  \n",
    "- $P(y = c)$: class prior  \n",
    "- $P(x)$: evidence (normalization term, same for all classes when we compare them)\n",
    "\n",
    "---\n",
    "\n",
    "### Class prior $P(y = c)$\n",
    "\n",
    "We estimate the prior by counting how often each class appears in the training set.\n",
    "Let $N_c$ be the number of samples with label $c$, and $N$ the total number of\n",
    "samples:\n",
    "\n",
    "$$\n",
    "P(y = c) \\approx \\hat P(y = c)\n",
    "= \\frac{N_c}{N} .\n",
    "$$\n",
    "\n",
    "So the prior just measures how common class $c$ is in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood of features $P(x \\mid y = c)$\n",
    "\n",
    "Let the feature vector be $x = (x_1, \\dots, x_d)$.\n",
    "\n",
    "1. **Naive conditional independence**\n",
    "\n",
    "   Given the class $y = c$, features are assumed conditionally independent:\n",
    "\n",
    "   $$\n",
    "   P(x \\mid y = c)\n",
    "   = \\prod_{j=1}^d P(x_j \\mid y = c) .\n",
    "   $$\n",
    "\n",
    "2. **Class-conditional Gaussian distributions**\n",
    "\n",
    "   For each class $c$ and feature $j$, we assume a univariate Gaussian:\n",
    "\n",
    "   $$\n",
    "   x_j \\mid y = c \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2),\n",
    "   $$\n",
    "\n",
    "   so the per-feature likelihood is\n",
    "\n",
    "   $$\n",
    "   P(x_j \\mid y = c)\n",
    "   = \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "   = \\frac{1}{\\sqrt{2\\pi\\sigma_{c,j}^2}}\n",
    "     \\exp\\!\\left(\n",
    "       -\\frac{(x_j - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\n",
    "     \\right).\n",
    "   $$\n",
    "\n",
    "   Combining these, the total likelihood is\n",
    "\n",
    "   $$\n",
    "   P(x \\mid y = c)\n",
    "   = \\prod_{j=1}^d\n",
    "     \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2).\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Final representation (prediction rule)\n",
    "\n",
    "For prediction we choose the class with the largest posterior probability:\n",
    "\n",
    "$$\n",
    "\\hat{y}\n",
    "= \\arg\\max_{c} P(y = c \\mid x).\n",
    "$$\n",
    "\n",
    "Using Bayes’ rule and dropping $P(x)$, which is the same for all classes,\n",
    "this becomes\n",
    "\n",
    "$$\n",
    "\\hat{y}\n",
    "= \\arg\\max_{c} P(x \\mid y = c)\\, P(y = c)\n",
    "= \\arg\\max_{c}\n",
    "\\left[\n",
    "  \\prod_{j=1}^d\n",
    "  \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "\\right]\n",
    "\\frac{N_c}{N}.\n",
    "$$\n",
    "\n",
    "In code, we usually work in **log space**, replacing products by sums:\n",
    "\n",
    "$$\n",
    "\\log P(y = c \\mid x)\n",
    "= \\log P(y = c)\n",
    "+ \\sum_{j=1}^d \\log \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "\\quad \\text{(up to an additive constant)} ,\n",
    "$$\n",
    "\n",
    "and still predict\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{c} \\log P(y = c \\mid x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351404c",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "We train Gaussian Naive Bayes by **maximum likelihood**,  \n",
    "which is equivalent to **minimizing the negative log-likelihood (NLL)**.\n",
    "\n",
    "---\n",
    "\n",
    "### General form\n",
    "\n",
    "Given a parametric model with parameters $\\Theta$ and training data\n",
    "$\\{(x_i, y_i)\\}_{i=1}^n$, the likelihood of the data is\n",
    "\n",
    "$$\n",
    "p_\\Theta(\\{x_i, y_i\\}_{i=1}^n)\n",
    "= \\prod_{i=1}^n p_\\Theta(y_i, x_i).\n",
    "$$\n",
    "\n",
    "The **negative log-likelihood loss** is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\Theta)\n",
    "= - \\sum_{i=1}^n \\log p_\\Theta(y_i, x_i).\n",
    "$$\n",
    "\n",
    "Minimizing $\\mathcal{L}_{\\text{NLL}}$ is the same as maximizing the likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "### NLL for Gaussian Naive Bayes\n",
    "\n",
    "For Gaussian NB, the joint model for a single sample is\n",
    "\n",
    "$$\n",
    "p_\\Theta(x_i, y_i)\n",
    "= \\pi_{y_i}\n",
    "  \\prod_{j=1}^d\n",
    "  \\mathcal{N}(x_{i,j} \\mid \\mu_{y_i,j}, \\sigma_{y_i,j}^2),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\pi_c = P(y = c)$ is the class prior,  \n",
    "- $\\mu_{c,j}$ and $\\sigma_{c,j}^2$ are the mean and variance of feature $j$ in class $c$.\n",
    "\n",
    "Plugging this into the general NLL and expanding the Gaussian log-density gives\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\Theta)\n",
    "=\n",
    "\\sum_{i=1}^n\n",
    "\\left[\n",
    "-\\log \\pi_{y_i}\n",
    "+\n",
    "\\sum_{j=1}^d\n",
    "\\left(\n",
    "\\frac{1}{2}\\log\\!\\big(2\\pi \\sigma_{y_i,j}^2\\big)\n",
    "+\n",
    "\\frac{(x_{i,j} - \\mu_{y_i,j})^2}{2\\sigma_{y_i,j}^2}\n",
    "\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "- The term $-\\log \\pi_{y_i}$ comes from the **class prior**.  \n",
    "- The inner sum over $j$ comes from the **Gaussian likelihood** of each feature.\n",
    "\n",
    "Because Gaussian NB has **closed-form MLE solutions** for $\\pi_c$, $\\mu_{c,j}$, and\n",
    "$\\sigma_{c,j}^2$, we usually do **not** run gradient descent on this loss in practice;\n",
    "instead we compute the empirical counts, means, and variances that minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99866d2a",
   "metadata": {},
   "source": [
    "## Optimizer(Not Real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830bf7a",
   "metadata": {},
   "source": [
    "Unlike discriminative models such as logistic regression or neural networks,  \n",
    "Gaussian Naive Bayes does not require an iterative optimizer.  \n",
    "Although the model minimizes the negative log-likelihood (NLL), the optimal parameters\n",
    "have closed-form maximum likelihood solutions.\n",
    "\n",
    "For each class $c$ and feature $j$, the MLE updates are:\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_c = \\frac{N_c}{N},\n",
    "$$\n",
    "where $N_c$ is the number of samples in class $c$.\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{c,j}\n",
    "= \\frac{1}{N_c} \\sum_{i : y_i = c} x_{i,j}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{c,j}^2\n",
    "= \\frac{1}{N_c} \\sum_{i : y_i = c} (x_{i,j} - \\hat{\\mu}_{c,j})^2.\n",
    "$$\n",
    "\n",
    "These formulas directly minimize the NLL loss, so training requires only computing\n",
    "class counts, sample means, and sample variances—no gradient descent, no iterative\n",
    "optimization, and no numerical solver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e4845",
   "metadata": {},
   "source": [
    "## Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4497a",
   "metadata": {},
   "source": [
    "Training Gaussian Naive Bayes model  \n",
    "**Require:** Training dataset $(X, y)$, smoothing parameter $\\text{var\\_smoothing}$  \n",
    "**Ensure:** Class priors $P(c)$, means $\\mu_{c,j}$, variances $\\sigma_{c,j}^2$  \n",
    "\n",
    "1:  $N \\leftarrow$ number of samples in $X$  \n",
    "2:  **for** $j = 1 \\dots d$ **do**  \n",
    "3:  $\\hat{\\sigma}_j^2 \\leftarrow \\frac{1}{N} \\sum_{i=1}^N (X_{i,j} - \\bar{X}_j)^2$  \n",
    "4:  **end for**  \n",
    "5:  $v_{\\max} \\leftarrow \\max_{1 \\le j \\le d} \\hat{\\sigma}_j^2$  \n",
    "6:  $\\epsilon \\leftarrow \\text{var\\_smoothing} \\cdot v_{\\max}$  \n",
    "\n",
    "7:  $C \\leftarrow \\text{unique}(y)$  \n",
    "8:  **for each** class $c \\in C$ **do**  \n",
    "9:   $S_c \\leftarrow \\{\\, i : y_i = c \\,\\}$  \n",
    "10:   $N_c \\leftarrow |S_c|$  \n",
    "11:   $P(c) \\leftarrow \\dfrac{N_c}{N}$  \n",
    "12:   **for** $j = 1 \\dots d$ **do**  \n",
    "13:    $X_{c,j} \\leftarrow \\{\\, X_{i,j} : i \\in S_c \\,\\}$  \n",
    "14:    $\\mu_{c,j} \\leftarrow \\dfrac{1}{N_c} \\sum_{i \\in S_c} X_{i,j}$  \n",
    "15:    $\\sigma_{c,j}^2 \\leftarrow \\dfrac{1}{N_c} \\sum_{i \\in S_c} (X_{i,j} - \\mu_{c,j})^2 + \\epsilon$  \n",
    "16:   **end for**  \n",
    "17: **end for**  \n",
    "\n",
    "---\n",
    "\n",
    "Predicting with Gaussian Naive Bayes  \n",
    "**Require:** Model parameters $P(c)$, $\\mu_{c,j}$, $\\sigma_{c,j}^2$  \n",
    "**Ensure:** Predicted label $\\hat{y}$  \n",
    "\n",
    "1:  **for each** class $c \\in C$ **do**  \n",
    "2:  $\\text{score}(c) \\leftarrow \\log P(c)$  \n",
    "3:  **for** $j = 1 \\dots d$ **do**  \n",
    "4:   $\\ell \\leftarrow -\\frac{1}{2}\\log(2\\pi\\sigma_{c,j}^2) \n",
    "      \\;-\\; \\frac{(x_j - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}$  \n",
    "5:   $\\text{score}(c) \\leftarrow \\text{score}(c) + \\ell$  \n",
    "6:  **end for**  \n",
    "7: **end for**  \n",
    "8:  $\\hat{y} \\leftarrow \\arg\\max_{c} \\text{score}(c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "252472ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier.\n",
    "\n",
    "    This classifier assumes all features follow a Gaussian distribution and\n",
    "    estimates per-class means and variances. Variance smoothing is applied\n",
    "    using the formula: epsilon = var_smoothing * max(feature_variances)\n",
    "\n",
    "    All probability calculations are done in log space to avoid numerical\n",
    "    underflow when probabilities become very small. During prediction the model\n",
    "    converts log probabilities back into normal probabilities and normalizes\n",
    "    them so that each row sums to one.\n",
    "\n",
    "    Feature likelihoods\n",
    "    ----------\n",
    "    For Gaussian numeric features:\n",
    "        log_pdf = -0.5 * ( log(2 * pi * variance) + ((x - mean)^2) / variance )\n",
    "\n",
    "    The model uses maximum likelihood estimates of the mean and variance\n",
    "    for each feature within each class, with an additional variance smoothing\n",
    "    term added for numerical stability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var_smoothing : float, optional (default=1e-9)\n",
    "        Portion of the largest feature variance added to each variance estimate\n",
    "        for numerical stability.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> X = np.array([[1.0, 2.0],\n",
    "    ...               [1.2, 1.9],\n",
    "    ...               [3.2, 4.8],\n",
    "    ...               [3.0, 5.1]])\n",
    "    >>> y = np.array(['A', 'A', 'B', 'B'])\n",
    "    >>> model = GaussianNaiveBayes()\n",
    "    >>> model.fit(X, y)\n",
    "    >>> model.predict([[1.1, 2.0]])\n",
    "    array(['A'], dtype='<U1')\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        \"\"\"\n",
    "        Initialize a Gaussian Naive Bayes classifier.\n",
    "\n",
    "        This constructor sets up the model hyperparameters and internal\n",
    "        data structures that will be populated during fitting.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_smoothing : float, default=1e-9\n",
    "            Non-negative smoothing factor added to feature variances\n",
    "            for numerical stability. The actual smoothing term is\n",
    "            scaled by the maximum variance across all features.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        classes_ : ndarray of shape (n_classes,)\n",
    "            Array of unique class labels observed during fitting.\n",
    "\n",
    "        log_priors_ : dict\n",
    "            Dictionary mapping each class label to its log prior\n",
    "            probability log P(Y = c).\n",
    "\n",
    "        gaussian_means_ : dict\n",
    "            Nested dictionary storing per-class per-feature Gaussian means.\n",
    "            gaussian_means_[c][j] = mean of feature j for class c.\n",
    "\n",
    "        gaussian_vars_ : dict\n",
    "            Nested dictionary storing per-class per-feature Gaussian variances\n",
    "            (after variance smoothing).\n",
    "            gaussian_vars_[c][j] = variance of feature j for class c.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - All attributes except var_smoothing are initialized empty and\n",
    "        filled during the call to fit().\n",
    "        \"\"\"\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.log_priors_ = {}\n",
    "        self.gaussian_means_ = {}\n",
    "        self.gaussian_vars_ = {}\n",
    "\n",
    "    # Likelihood helper functions\n",
    "    def _gaussian_log_pdf(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Compute the log probability density of a Gaussian distribution.\n",
    "\n",
    "        This function evaluates the log of the Gaussian (normal) probability\n",
    "        density function for a single feature value.\n",
    "\n",
    "        The Gaussian log-PDF is given by:\n",
    "            log N(x | mean, var)\n",
    "            = -0.5 * [ log(2π · var) + (x - mean)^2 / var ]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float\n",
    "            Observed feature value.\n",
    "\n",
    "        mean : float\n",
    "            Mean of the Gaussian distribution for the given class and feature.\n",
    "\n",
    "        var : float\n",
    "            Variance of the Gaussian distribution for the given class and feature.\n",
    "            Assumed to be strictly positive after variance smoothing.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Log probability density log P(x | mean, var).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - Computation is done entirely in log space for numerical stability.\n",
    "        - This method is used as a building block for computing class-conditional\n",
    "        log-likelihoods.\n",
    "        \"\"\"\n",
    "        return -0.5 * (np.log(2.0 * np.pi * var) + ((x - mean) ** 2) / var)\n",
    "\n",
    "    # Aggregator\n",
    "    def _compute_log_likelihood(self, x_row, class_label):\n",
    "        \"\"\"\n",
    "        Compute the total log-likelihood log p(x | c) of a single sample under\n",
    "        a given class using the Gaussian Naive Bayes assumption.\n",
    "\n",
    "        Under the Naive Bayes conditional independence (i.i.d.) assumption,\n",
    "        the joint likelihood factorizes across features:\n",
    "            p(x | c) = ∏_j p(x_j | c)\n",
    "            log p(x | c) = ∑_j log p(x_j | c)\n",
    "\n",
    "        Each feature likelihood p(x_j | c) is modeled as a univariate Gaussian\n",
    "        distribution with class-specific mean and variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_row : array-like of shape (n_features,)\n",
    "            A single data point.\n",
    "        class_label : object\n",
    "            The class for which likelihood is being computed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The total log-likelihood log p(x | c), computed as the sum of \n",
    "            Gaussian log-densities across all features.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "            - This method operates fully in log space to ensure numerical stability.\n",
    "            - The method assumes that Gaussian means and variances for each class\n",
    "            have already been estimated and stored in:\n",
    "                - self.gaussian_means_[class_label]\n",
    "                - self.gaussian_vars_[class_label]\n",
    "\n",
    "            Algorithm\n",
    "            ---------\n",
    "            1. Initialize log-likelihood accumulator log_l = 0\n",
    "            2. For each feature j:\n",
    "                a. Retrieve class-specific mean μ_{c,j} and variance σ²_{c,j}\n",
    "                b. Compute log p(x_j | c) using a Gaussian log-pdf\n",
    "                c. Add the result to log_l\n",
    "            3. Return the accumulated log-likelihood\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0}}\n",
    "        >>> model.gaussian_vars_ = {'A': {0: 1.0}}\n",
    "        >>> model._compute_log_likelihood([0.0], 'A')\n",
    "        0.0\n",
    "\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0, 1: 1.0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1.0, 1: 1.0}}\n",
    "        >>> model._compute_log_likelihood([0.0, 1.0], 'A')\n",
    "        -1.8378770664093453\n",
    "        \"\"\"\n",
    "        log_l = 0.0\n",
    "\n",
    "        for j in range(len(x_row)):\n",
    "            mean = self.gaussian_means_[class_label][j]\n",
    "            var  = self.gaussian_vars_[class_label][j]\n",
    "            log_l += self._gaussian_log_pdf(x_row[j], mean, var)\n",
    "\n",
    "        return log_l\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Naive Bayes model using training data.\n",
    "\n",
    "        This method estimates:\n",
    "        - Class prior probabilities P(Y = c)\n",
    "        - Per-class per-feature Gaussian parameters (mean and variance)\n",
    "\n",
    "        under the Naive Bayes conditional independence assumption.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training feature matrix consisting of continuous numerical features.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Class labels corresponding to each training sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the fitted model instance.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If X contains missing values (NaN). This implementation assumes\n",
    "            all features are fully observed.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - Variance smoothing is applied to each feature variance using:\n",
    "            var_jc ← var_jc + var_smoothing * max_j Var(X_j)\n",
    "        where Var(X_j) is the variance of feature j across the entire dataset.\n",
    "        - All probability-related quantities are stored in log space.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        if np.isnan(X).any():\n",
    "            raise ValueError(\"GaussianNaiveBayes does not support missing values. \"\n",
    "                     \"Please remove or impute missing data before fitting.\")\n",
    "\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        feature_vars = X.var(axis=0)          # per-feature variance\n",
    "        max_var = np.max(feature_vars)        # biggest variance across features\n",
    "        epsilon = self.var_smoothing * max_var \n",
    "\n",
    "        # class priors\n",
    "        for c in self.classes_:\n",
    "            n_c = np.sum(y == c)\n",
    "            self.log_priors_[c] = np.log(n_c / n_samples)\n",
    "\n",
    "        # init Gaussian dicts\n",
    "        for c in self.classes_:\n",
    "            self.gaussian_means_[c] = {}\n",
    "            self.gaussian_vars_[c] = {}\n",
    "\n",
    "        # compute Gaussian parameters\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y == c]\n",
    "\n",
    "            means = Xc.mean(axis=0)\n",
    "            vars_  = Xc.var(axis=0)\n",
    "\n",
    "            # smoothing:\n",
    "            # var += eps * global_variance\n",
    "            smoothed_vars = vars_ + epsilon\n",
    "\n",
    "            for j in range(n_features):\n",
    "                self.gaussian_means_[c][j] = means[j]\n",
    "                self.gaussian_vars_[c][j] = smoothed_vars[j]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Prediction\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute unnormalized log posterior scores log P(Y = c | X) for each sample\n",
    "        and each class using Gaussian Naive Bayes.\n",
    "\n",
    "        Using Bayes' rule, the posterior is proportional to the product of the\n",
    "        class prior and the class-conditional likelihood:\n",
    "            P(c | x) ∝ P(c) · P(x | c)\n",
    "\n",
    "        Taking the logarithm gives:\n",
    "            log P(c | x) = log P(c) + log P(x | c)\n",
    "\n",
    "        where log P(x | c) is computed under the Naive Bayes conditional\n",
    "        independence (i.i.d.) assumption as the sum of per-feature Gaussian\n",
    "        log-likelihoods.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples, where each row corresponds to a single data point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Log posterior scores (not normalized).\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        - All computations are performed in log space for numerical stability.\n",
    "        - This method assumes that the following attributes have already been\n",
    "        estimated during fitting:\n",
    "            - self.classes_\n",
    "            - self.log_priors_\n",
    "            - self.gaussian_means_\n",
    "            - self.gaussian_vars_\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Initialize a matrix log_probs of shape (n_samples, n_classes)\n",
    "        2. For each sample i:\n",
    "            a. Extract the sample x_row = X[i]\n",
    "            b. For each class c:\n",
    "                i.   Compute log-likelihood log P(x | c) using _compute_log_likelihood\n",
    "                ii.  Add the class log prior log P(c)\n",
    "                iii. Store the result in log_probs[i, c]\n",
    "        3. Return the log_probs matrix            \n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1.0}}\n",
    "        >>> model.predict_log_proba([[0]])\n",
    "        array([[0.]])\n",
    "\n",
    "        >>> model.classes_ = np.array(['A', 'B'])\n",
    "        >>> model.log_priors_ = {'A': np.log(0.5), 'B': np.log(0.5)}\n",
    "        >>> model.gaussian_means_ = {\n",
    "        ...     'A': {0: 0.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.gaussian_vars_ = {\n",
    "        ...     'A': {0: 1.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.predict_log_proba([[0.0], [1.0]])\n",
    "        array([[-0.9189..., -1.4189...],\n",
    "            [-1.4189..., -0.9189...]])\n",
    "            \n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, len(self.classes_)))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_row = X[i]\n",
    "            for k, c in enumerate(self.classes_):\n",
    "                log_l = self._compute_log_likelihood(x_row, c)\n",
    "                log_probs[i, k] = self.log_priors_[c] + log_l\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute normalized posterior probabilities P(Y = c | X) for each sample\n",
    "        using the Gaussian Naive Bayes model.\n",
    "\n",
    "        This method converts unnormalized log posterior scores obtained from\n",
    "        `predict_log_proba` into valid probability distributions by applying\n",
    "        the softmax function. To ensure numerical stability, the log-sum-exp\n",
    "        trick is used by shifting log probabilities before exponentiation.\n",
    "\n",
    "        Specifically, for each sample x:\n",
    "            P(c | x) = exp(log P(c | x)) / sum_k exp(log P(k | x))\n",
    "        where log P(c | x) = log P(c) + log P(x | c).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Normalized probabilities. Each row sums to 1.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - This method relies on `predict_log_proba` to compute unnormalized\n",
    "        log posterior scores.\n",
    "        - For numerical stability, the maximum log posterior value is subtracted\n",
    "        from each row before exponentiation:\n",
    "            shifted = log_probs - max(log_probs)\n",
    "\n",
    "        This does not change the resulting probabilities because softmax\n",
    "        is invariant to constant shifts.\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Compute unnormalized log posterior scores using predict_log_proba(X)\n",
    "        2. For each sample, subtract the maximum log score (numerical stability)\n",
    "        3. Exponentiate the shifted log scores\n",
    "        4. Normalize by dividing by the row-wise sum\n",
    "        5. Return normalized probabilities\n",
    "\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1}}\n",
    "        >>> model.predict_proba([[0]])\n",
    "        array([[1.]])\n",
    "\n",
    "        >>> model.classes_ = np.array(['A', 'B'])\n",
    "        >>> model.log_priors_ = {'A': np.log(0.5), 'B': np.log(0.5)}\n",
    "        >>> model.gaussian_means_ = {\n",
    "        ...     'A': {0: 0.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.gaussian_vars_ = {\n",
    "        ...     'A': {0: 1.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.predict_proba([[0.0], [1.0]])\n",
    "        array([[0.6224..., 0.3775...],\n",
    "            [0.3775..., 0.6224...]])\n",
    "        \"\"\"\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        max_log = np.max(log_probs, axis=1, keepdims=True)\n",
    "        shifted = log_probs - max_log\n",
    "\n",
    "        probs = np.exp(shifted)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the most likely class label for each sample.\n",
    "\n",
    "        This method first computes normalized posterior probabilities using\n",
    "        `predict_proba`, and then selects the class with the highest probability\n",
    "        for each sample. Formally, the predicted class is:\n",
    "            y_hat = argmax_c P(Y = c | x)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "            Predicted class labels corresponding to the maximum posterior\n",
    "            probability for each sample.\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Compute normalized posterior probabilities using predict_proba(X)\n",
    "        2. Find the index of the maximum probability for each sample\n",
    "        3. Map indices to class labels using self.classes_\n",
    "        4. Return predicted class labels\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> X = np.array([[0], [2]])\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1}}\n",
    "        >>> model.predict(X)\n",
    "        array(['A', 'A'], dtype='<U1')\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        class_indices = np.argmax(probs, axis=1)\n",
    "        return self.classes_[class_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0d839",
   "metadata": {},
   "source": [
    "## Result comparison with sklearn Gaussian Naive Bayes\n",
    "\n",
    "To validate the correctness of our implementation, we compare it with sklearn.naive_bayes.GaussianNB on the Breast Cancer Wisconsin dataset. This dataset contains a binary target variable (malignant vs. benign) and 30 continuous numerical features, which makes it suitable for Gaussian Naive Bayes. The dataset is moderate in size (569 samples) and requires minimal preprocessing. This allows us to focus on verifying the correctness of the model implementation rather than handling extensive data cleaning or feature engineering, making it a good benchmark dataset for algorithm validation and comparison.\n",
    "\n",
    "We apply the same preprocessing steps for both models: dropping the id column, mapping the binary target variable (M → 1, B → 0), removing missing values, and splitting the data into training and test sets using an 80/20 split with random_state=42 and stratify=y to preserve class proportions. Both models are trained on the identical training set, and we evaluate performance on the test set using accuracy and additionally compare predictions on a per-sample basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d4bb956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My accuracy: 0.9385964912280702\n",
      "sklearn accuracy: 0.9385964912280702\n",
      "Number of difference: 0\n",
      "Different number index: []\n",
      "True y: []\n",
      "My y:       []\n",
      "sklearn y:  []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"data/data.csv\")\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({\"M\": 1, \"B\": 0})\n",
    "df = df.dropna()\n",
    "\n",
    "X = df.drop(columns=[\"id\", \"diagnosis\"]).values.astype(float)\n",
    "y = df[\"diagnosis\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "my_nb = GaussianNaiveBayes()\n",
    "my_nb.fit(X_train, y_train)\n",
    "y_pred_my = my_nb.predict(X_test)\n",
    "\n",
    "sk_nb = GaussianNB(var_smoothing=1e-9)\n",
    "sk_nb.fit(X_train, y_train)\n",
    "y_pred_sk = sk_nb.predict(X_test)\n",
    "\n",
    "print(\"My accuracy:\", accuracy_score(y_test, y_pred_my))\n",
    "print(\"sklearn accuracy:\", accuracy_score(y_test, y_pred_sk))\n",
    "\n",
    "diff_mask = (y_pred_my != y_pred_sk)\n",
    "num_diff = diff_mask.sum()\n",
    "print(\"Number of difference:\", num_diff)\n",
    "\n",
    "diff_indices = np.where(diff_mask)[0]\n",
    "print(\"Different number index:\", diff_indices)\n",
    "\n",
    "print(\"True y:\", y_test[diff_indices])\n",
    "print(\"My y:      \", y_pred_my[diff_indices])\n",
    "print(\"sklearn y: \", y_pred_sk[diff_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b17b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability comparison PASSED: predicted probabilities match sklearn within numerical tolerance.\n"
     ]
    }
   ],
   "source": [
    "probs_my = my_nb.predict_proba(X_test)\n",
    "probs_sk = sk_nb.predict_proba(X_test)\n",
    "\n",
    "prob_diff = np.abs(probs_my - probs_sk)\n",
    "max_diff = prob_diff.max()\n",
    "\n",
    "assert np.allclose(probs_my, probs_sk, atol=1e-10), \"Predicted probabilities differ from sklearn beyond tolerance.\"\n",
    "\n",
    "print(\"Probability comparison PASSED: predicted probabilities match sklearn within numerical tolerance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfdf82",
   "metadata": {},
   "source": [
    "The results show that our implementation and sklearn’s implementation achieve exactly the same accuracy (0.9386), and there are zero differences in predicted class labels across all test samples. In addition, the posterior class probabilities produced by both models also match for every test instance. Together, these results demonstrate that our Gaussian Naive Bayes model not only matches sklearn in aggregate performance, but also exactly reproduces both its predictions and probabilistic outputs on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5c170",
   "metadata": {},
   "source": [
    "## Check Model\n",
    "\n",
    "In this section, we validate the correctness and robustness of our Gaussian Naive Bayes implementation through a set of unit tests to ensure that every function works correctly, and also under edge cases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69400fff",
   "metadata": {},
   "source": [
    "# Check 1: `_gaussian_log_pdf`\n",
    "\n",
    "**Test 1.1 : Gaussian log pdf at the mean**\n",
    "\n",
    "checks that our implementation of `_gaussian_log_pdf` matches the closed-form analytical value at the mean.  \n",
    "\n",
    "For $N(0, 1)$, log pdf at $x=0$ is $-0.5 * \\log(2*pi)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4fd1ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed: -0.9189385332046727\n",
      "Expected: -0.9189385332046727\n",
      "Test 1.1 PASSED: gaussian_log_pdf center value correct.\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "m = GaussianNaiveBayes()\n",
    "\n",
    "val = m._gaussian_log_pdf(0.0, mean=0.0, var=1.0)\n",
    "\n",
    "expected = -0.5 * np.log(2.0 * np.pi * 1.0)\n",
    "\n",
    "print(\"Computed:\", val)\n",
    "print(\"Expected:\", expected)\n",
    "assert val == pytest.approx(expected, rel=1e-6)\n",
    "print(\"Test 1.1 PASSED: gaussian_log_pdf center value correct.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8160a",
   "metadata": {},
   "source": [
    "**Test 1.2: Monotonic decay away from the mean**\n",
    "\n",
    "checks that our implementation of `_gaussian_log_pdf` gives correct Gaussian log density result that will monotonic decay as we move away from the mean.\n",
    "\n",
    "For a fixed normal distribution $N(0, 1)$, the log pdf should be highest at the mean and decrease as $|x - \\mu|$ increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c5e9aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9189385332046727 -1.4189385332046727 -2.9189385332046727\n",
      "Test 1.2 PASSED: gaussian_log_pdf monotonicity correct.\n"
     ]
    }
   ],
   "source": [
    "center = m._gaussian_log_pdf(0.0, mean=0.0, var=1.0)\n",
    "off_1  = m._gaussian_log_pdf(1.0, mean=0.0, var=1.0)\n",
    "off_2  = m._gaussian_log_pdf(2.0, mean=0.0, var=1.0)\n",
    "print(center, off_1, off_2)\n",
    "\n",
    "assert center > off_1 > off_2\n",
    "print(\"Test 1.2 PASSED: gaussian_log_pdf monotonicity correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036c767",
   "metadata": {},
   "source": [
    "**Test 1.3: Effect of the variance on the log pdf**\n",
    "\n",
    "checks that changing the variance parameter in the Gaussian distribution changes the output of `_gaussian_log_pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd673bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1.3 PASSED: variance affects gaussian_log_pdf output.\n"
     ]
    }
   ],
   "source": [
    "n1 = m._gaussian_log_pdf(1.0, mean=0.0, var=0.5)\n",
    "n2 = m._gaussian_log_pdf(1.0, mean=0.0, var=5.0)\n",
    "\n",
    "assert not np.isclose(n1, n2)\n",
    "print(\"Test 1.3 PASSED: variance affects gaussian_log_pdf output.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaabe739",
   "metadata": {},
   "source": [
    "# Check 2: `_compute_log_likelihood`\n",
    "\n",
    "**Test 2.1: calculation of log likelihood value**\n",
    "\n",
    "checks if `_compute_log_likelihood` correctly sums the per-feature Gaussian log pdfs by manually calculating total log-likelihood from a 2-dimensional toy dataset example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61a0ebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2.1 PASSED: compute_log_likelihood matches manual calculation.\n"
     ]
    }
   ],
   "source": [
    "m.classes_ = np.array([0])\n",
    "m.gaussian_means_[0] = {0: 0.0, 1: 1.0}\n",
    "m.gaussian_vars_[0]  = {0: 1.0, 1: 4.0}\n",
    "\n",
    "x_row = np.array([0.0, 3.0])\n",
    "log1 = m._gaussian_log_pdf(x_row[0], 0.0, 1.0)\n",
    "log2 = m._gaussian_log_pdf(x_row[1], 1.0, 4.0)\n",
    "expected = log1 + log2\n",
    "\n",
    "assert m._compute_log_likelihood(x_row, 0) == pytest.approx(expected, rel=1e-6)\n",
    "print(\"Test 2.1 PASSED: compute_log_likelihood matches manual calculation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c1e79",
   "metadata": {},
   "source": [
    "**Test 2.2： distance and log-likelihood**\n",
    "\n",
    "checks that samples farther from the class mean produce lower log-likelihood values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "490f2ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2.2 PASSED: log-likelihood decreases for far-away samples.\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([0.0, 1.0])\n",
    "x2 = np.array([5.0, 10.0])\n",
    "\n",
    "ll1 = m._compute_log_likelihood(x1, 0)\n",
    "ll2 = m._compute_log_likelihood(x2, 0)\n",
    "\n",
    "assert ll1 > ll2\n",
    "print(\"Test 2.2 PASSED: log-likelihood decreases for far-away samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8649ec",
   "metadata": {},
   "source": [
    "# Check 3: `fit`\n",
    "\n",
    "**Test 3.1: Correct computation of class priors and class structure**\n",
    "\n",
    "checks that `fit()` correctly detects class labels and calculates the corresponding log-priors based on class frequencies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b8e607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3.1 PASSED: fit() correctly computes priors and classes.\n"
     ]
    }
   ],
   "source": [
    "X_toy = np.array([[1,2],[1.2,1.9],[3.2,4.8],[3,5.1],[2.5,2.9]])\n",
    "y_toy = np.array([0,0,1,1,1])\n",
    "\n",
    "m.fit(X_toy, y_toy)\n",
    "\n",
    "# classes\n",
    "assert isinstance(m.classes_, np.ndarray)\n",
    "assert m.classes_.ndim == 1\n",
    "assert set(m.classes_) == {0,1}\n",
    "\n",
    "assert set(m.log_priors_.keys()) == set(m.classes_)\n",
    "\n",
    "\n",
    "N = len(y_toy)\n",
    "for c in m.classes_:\n",
    "    expected = np.log(np.sum(y_toy == c) / N)\n",
    "    assert np.isclose(m.log_priors_[c], expected)\n",
    "\n",
    "print(\"Test 3.1 PASSED: fit() correctly computes priors and classes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee2186",
   "metadata": {},
   "source": [
    "**Test 3.2: Positive variances after smoothing**\n",
    "\n",
    "checks that all per-feature variances are strictly positive after smoothing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac8fdba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3.2 PASSED: fit() produces positive variances.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for c in m.classes_:\n",
    "    for j in range(X_toy.shape[1]):\n",
    "        assert m.gaussian_vars_[c][j] > 0\n",
    "\n",
    "print(\"Test 3.2 PASSED: fit() produces positive variances.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57376c61",
   "metadata": {},
   "source": [
    "**Test 3.3: Correct structure of stored means and variances**\n",
    "\n",
    "checks that each class stores a full set of per-feature means and variances with the correct dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41fc5cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3.3 PASSED: fit() structure for means/vars correct.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_features = X_toy.shape[1]\n",
    "\n",
    "for c in m.classes_:\n",
    "    assert len(m.gaussian_means_[c]) == n_features\n",
    "    assert len(m.gaussian_vars_[c]) == n_features\n",
    "\n",
    "print(\"Test 3.3 PASSED: fit() structure for means/vars correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518cd32",
   "metadata": {},
   "source": [
    "**Test 3.4: NaN in input should raise ValueError**\n",
    "\n",
    "when there's NaN value in fit() input, raise ValueError. This ValueError is included in the model since GNB could not process NaN values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bfbc5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3.4 PASSED: fit raises ValueError when X contains NaN.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    X_nan = np.array([[1.0, 2.0],\n",
    "                      [np.nan, 3.0]])\n",
    "    y_nan = np.array([0, 1])\n",
    "\n",
    "    m.fit(X_nan, y_nan)\n",
    "    raise AssertionError(\"Test 3.4 FAILED: fit did not raise ValueError on NaN input.\")\n",
    "except ValueError:\n",
    "    print(\"Test 3.4 PASSED: fit raises ValueError when X contains NaN.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953004a",
   "metadata": {},
   "source": [
    "# Check 4: `predict_log_proba`\n",
    "\n",
    "**Test 4.1: Output shape and dtype of `predict_log_proba`**\n",
    "\n",
    "checks that `predict_log_proba` method returns the output shape (n_sample, n_classes) and dtype.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfe88d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.1 PASSED: predict_log_proba shape + dtype correct.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = np.array([[1.1,2.0],[3.1,4.9]])\n",
    "\n",
    "logp = m.predict_log_proba(X_test)\n",
    "assert isinstance(logp, np.ndarray)\n",
    "assert logp.shape == (2,2)\n",
    "assert logp.dtype == float\n",
    "\n",
    "print(\"Test 4.1 PASSED: predict_log_proba shape + dtype correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c90e7",
   "metadata": {},
   "source": [
    "**Test 4.2: `predict_log_proba` scores**\n",
    "\n",
    "checks that the more distant the samples, the lower the log-posterior scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71d80317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4.2 PASSED: predict_log_proba scores changes correspond to the distance.\n"
     ]
    }
   ],
   "source": [
    "lp1 = m.predict_log_proba([[1.1,2.0]])[0]\n",
    "lp2 = m.predict_log_proba([[100,100]])[0]\n",
    "\n",
    "assert lp1.max() > lp2.max()\n",
    "print(\"Test 4.2 PASSED: predict_log_proba scores changes correspond to the distance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3011f",
   "metadata": {},
   "source": [
    "# Check 5: `predict_proba`\n",
    "\n",
    "**Test 5.1: Probability normalization**\n",
    "\n",
    "check that each probability row from `predict_proba` sums to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9669de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5.1 PASSED: predict_proba rows sum to 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = m.predict_proba(X_test)\n",
    "assert np.allclose(probs.sum(axis=1), 1.0)\n",
    "\n",
    "print(\"Test 5.1 PASSED: predict_proba rows sum to 1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a4510",
   "metadata": {},
   "source": [
    "**Test 5.2: Non negative probability**\n",
    "\n",
    "checks all values returned by `predict_proba` are non-negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "720bf0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5.2 PASSED: predict_proba produces non-negative probability.\n"
     ]
    }
   ],
   "source": [
    "assert np.all(probs >= 0)\n",
    "print(\"Test 5.2 PASSED: predict_proba produces non-negative probability.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b35ed11",
   "metadata": {},
   "source": [
    "**Test 5.3: Consistency between log-probability and probability**\n",
    "\n",
    "checks that the class with the highest log-probability also has the highest probability after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cdc53d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5.3 PASSED: predict_proba aligns with predict_log_proba.\n"
     ]
    }
   ],
   "source": [
    "# class with max log-prob = class with max prob\n",
    "logp = m.predict_log_proba(X_test)\n",
    "probs = m.predict_proba(X_test)\n",
    "\n",
    "assert np.argmax(logp[0]) == np.argmax(probs[0])\n",
    "print(\"Test 5.3 PASSED: predict_proba aligns with predict_log_proba.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab17e1",
   "metadata": {},
   "source": [
    "# Check 6: `predict`\n",
    "\n",
    "**Test 6.1: Output shape and valid class labels**\n",
    "\n",
    "checks that `predict` returns a 1D array of valid class labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71f503fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6.1 PASSED: predict shape and value set correct.\n"
     ]
    }
   ],
   "source": [
    "preds = m.predict(X_test)\n",
    "\n",
    "assert preds.shape == (X_test.shape[0],)\n",
    "assert set(preds).issubset(set(y_toy))\n",
    "\n",
    "print(\"Test 6.1 PASSED: predict shape and value set correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab9267",
   "metadata": {},
   "source": [
    "**Test 6.2: Consistency with `predict_proba`**\n",
    "\n",
    "checks that `predict` always chooses the class with the highest predicted probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1fe9a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6.2 PASSED: predict matches argmax of probability.\n"
     ]
    }
   ],
   "source": [
    "probs = m.predict_proba(X_test)\n",
    "assert np.array_equal(preds, np.argmax(probs, axis=1))\n",
    "\n",
    "print(\"Test 6.2 PASSED: predict matches argmax of probability.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd131b32",
   "metadata": {},
   "source": [
    "# Check 7: Edge cases\n",
    "\n",
    "**Test 7.1: zero-variance feature**\n",
    "\n",
    "checks the case where a feature has variance 0 within each class. Confirms variance smoothing prevents divide-by-zero and that predict_log_proba()/predict_proba() remain finite and normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40c6dfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7.1 PASSED: zero-variance features are handled correctly via variance smoothing.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_zero_var = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [1.0, 3.0],\n",
    "    [1.0, 4.0],\n",
    "    [1.0, 5.0],\n",
    "])\n",
    "y_zero_var = np.array([0, 0, 1, 1])\n",
    "\n",
    "m.fit(X_zero_var, y_zero_var)\n",
    "\n",
    "logp_zero = m.predict_log_proba(X_zero_var)\n",
    "probs_zero = m.predict_proba(X_zero_var)\n",
    "\n",
    "assert np.isfinite(logp_zero).all(), \"Test 7.1 FAILED: predict_log_proba produced non-finite values with zero-variance feature.\"\n",
    "assert np.isfinite(probs_zero).all(), \"Test 7.1 FAILED: predict_proba produced non-finite values with zero-variance feature.\"\n",
    "assert np.allclose(probs_zero.sum(axis=1), 1.0, atol=1e-6), \"Test 7.1 FAILED: probabilities do not sum to ~1.\"\n",
    "\n",
    "print(\"Test 7.1 PASSED: zero-variance features are handled correctly via variance smoothing.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592b718",
   "metadata": {},
   "source": [
    "**Test 7.2: Single-class training**\n",
    "\n",
    "checks that training on data with only one class does not crash and produces reasonable outputs. Ensures classes_ has length 1, predict_proba() returns probabilities ≈ 1, and predict() always returns the only class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac20e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7.2 PASSED: model behaves correctly when trained on a single class.\n"
     ]
    }
   ],
   "source": [
    "X_one_class = np.array([\n",
    "    [1.0, 2.0],\n",
    "    [1.5, 2.5],\n",
    "    [2.0, 3.0],\n",
    "])\n",
    "y_one_class = np.array([0, 0, 0])\n",
    "\n",
    "m.fit(X_one_class, y_one_class)\n",
    "\n",
    "assert m.classes_.shape == (1,), \"Test 7.2 FAILED: classes_ should contain exactly one class.\"\n",
    "assert m.classes_[0] == 0, \"Test 7.2 FAILED: the single class in classes_ should be 0.\"\n",
    "\n",
    "probs_one_class = m.predict_proba(X_one_class)\n",
    "preds_one_class = m.predict(X_one_class)\n",
    "\n",
    "assert probs_one_class.shape == (X_one_class.shape[0], 1), \"Test 7.2 FAILED: predict_proba shape incorrect for single-class training.\"\n",
    "assert np.allclose(probs_one_class, 1.0, atol=1e-6), \"Test 7.2 FAILED: probabilities are not all ~1 for single-class training.\"\n",
    "assert np.array_equal(preds_one_class, np.zeros_like(y_one_class)), \"Test 7.2 FAILED: predict did not return the only class.\"\n",
    "\n",
    "print(\"Test 7.2 PASSED: model behaves correctly when trained on a single class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd5a09",
   "metadata": {},
   "source": [
    "**Test 7.3: Single test sample**\n",
    "\n",
    "verifies shape handling and probability normalization when predicting on a single input sample. Ensures predict_proba() returns shape (1, C), sums to 1, and predict() returns a valid class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47347d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7.3 PASSED: predict_proba/predict handle a single test sample correctly.\n"
     ]
    }
   ],
   "source": [
    "X_train_small = np.array([\n",
    "    [2.0, 3.0],\n",
    "    [3.0, 4.0],\n",
    "    [4.0, 5.0],\n",
    "])\n",
    "y_train_small = np.array([0, 1, 1])\n",
    "X_test_single = np.array([[3.5, 4.5]])\n",
    "\n",
    "m.fit(X_train_small, y_train_small)\n",
    "\n",
    "probs_test_single = m.predict_proba(X_test_single)\n",
    "pred_test_single = m.predict(X_test_single)\n",
    "\n",
    "assert probs_test_single.shape == (1, 2), \"Test 7.3 FAILED: predict_proba shape incorrect for a single test sample.\"\n",
    "assert np.allclose(probs_test_single.sum(axis=1), 1.0, atol=1e-6), \"Test 7.3 FAILED: probs do not sum to ~1.\"\n",
    "assert pred_test_single.shape == (1,), \"Test 7.3 FAILED: predict shape incorrect for a single test sample.\"\n",
    "assert pred_test_single[0] in m.classes_, \"Test 7.3 FAILED: predicted label not in known classes.\"\n",
    "\n",
    "print(\"Test 7.3 PASSED: predict_proba/predict handle a single test sample correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddb672",
   "metadata": {},
   "source": [
    "**Test 7.4: Extreme magnitudes**\n",
    "\n",
    "tests numerical stability under very large feature values. Confirms log-sum-exp and log-space computations avoid underflow/overflow and keep outputs finite and normalized.\n",
    "\n",
    "Large values stress (x-mean)^2 / var. We only require finite outputs and normalized probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c4bf4854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7.4 PASSED: extreme magnitudes remain numerically stable (log-space + log-sum-exp).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_extreme = np.array([\n",
    "    [ 1e9,  -1e9],\n",
    "    [ 1e9+1, -1e9-1],\n",
    "    [-1e9,   1e9],\n",
    "    [-1e9-1, 1e9+1],\n",
    "])\n",
    "y_extreme = np.array([0, 0, 1, 1])\n",
    "\n",
    "m.fit(X_extreme, y_extreme)\n",
    "\n",
    "logp_ext = m.predict_log_proba(X_extreme)\n",
    "probs_ext = m.predict_proba(X_extreme)\n",
    "\n",
    "assert np.isfinite(logp_ext).all(), \"Test 7.4 FAILED: log-probabilities contain NaN/inf under extreme magnitudes.\"\n",
    "assert np.isfinite(probs_ext).all(), \"Test 7.4 FAILED: probabilities contain NaN/inf under extreme magnitudes.\"\n",
    "assert np.allclose(probs_ext.sum(axis=1), 1.0, atol=1e-6), \"Test 7.4 FAILED: probs do not sum to ~1 under extreme magnitudes.\"\n",
    "\n",
    "print(\"Test 7.4 PASSED: extreme magnitudes remain numerically stable (log-space + log-sum-exp).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b755a",
   "metadata": {},
   "source": [
    "Overall, these tests demonstrates that each component of our Gaussian Naive Bayes implementation works correctly in isolation, handles edge cases correctly, and exactly reproduces both the predictions and probabilistic outputs of sklearn’s implementation on a real-world dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data2060]",
   "language": "python",
   "name": "conda-env-data2060-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
