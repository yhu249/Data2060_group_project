{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a93013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MixedNaiveBayes:\n",
    "    \"\"\"\n",
    "    A Naive Bayes classifier that supports mixed feature types:\n",
    "    - Gaussian (continuous numerical features)\n",
    "    - Bernoulli (binary 0/1 features)\n",
    "    - Categorical (multi-class discrete features)\n",
    "    - Multinomial (count-based features)\n",
    "\n",
    "    The classifier computes all likelihoods in log space to avoid\n",
    "    numerical underflow and applies Laplace smoothing to Bernoulli,\n",
    "    Categorical, and Multinomial likelihoods.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Class priors P(Y=c) are computed WITHOUT Laplace smoothing.\n",
    "    - Feature likelihoods P(X_j | Y=c) use Laplace smoothing for\n",
    "      Bernoulli, Categorical, and Multinomial features.\n",
    "    - Gaussian features use MLE mean and variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eps : float, optional\n",
    "        Small constant added to Gaussian variances to avoid zero variance.\n",
    "    laplace_alpha : float, optional\n",
    "        Laplace smoothing parameter for Bernoulli, Categorical, and Multinomial features.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> X = np.array([[5.1, 1, \"red\"],\n",
    "    ...               [4.9, 0, \"blue\"],\n",
    "    ...               [5.0, 1, \"red\"]])\n",
    "    >>> y = np.array([\"A\", \"B\", \"A\"])\n",
    "    >>> feature_types = [\"gaussian\", \"bernoulli\", \"categorical\"]\n",
    "    >>> model = MixedNaiveBayes()\n",
    "    >>> model.fit(X, y, feature_types)\n",
    "    >>> model.predict(X)\n",
    "    array(['A', 'B', 'A'], dtype='<U1')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-8, laplace_alpha=1.0):\n",
    "        \"\"\"\n",
    "        Initialize model parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eps : float\n",
    "            Stability constant added to Gaussian variances.\n",
    "        laplace_alpha : float\n",
    "            Laplace smoothing strength for Bernoulli, Categorical, and Multinomial features.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = MixedNaiveBayes(eps=1e-6, laplace_alpha=1.0)\n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "        self.alpha = laplace_alpha\n",
    "\n",
    "        self.feature_types = None\n",
    "        self.classes_ = None\n",
    "\n",
    "        # learned parameters\n",
    "        self.log_priors_ = {}\n",
    "        self.gaussian_means_ = {}\n",
    "        self.gaussian_vars_ = {}\n",
    "        self.bernoulli_probs_ = {}\n",
    "        self.categorical_probs_ = {}\n",
    "        self.categorical_values_ = {}\n",
    "        self.multinomial_probs_ = {}\n",
    "\n",
    "    #  ikelihood functions\n",
    "    def _gaussian_log_pdf(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Compute log density of a univariate Gaussian.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float\n",
    "            Observed feature value.\n",
    "        mean : float\n",
    "            Gaussian mean for class c and feature j.\n",
    "        var : float\n",
    "            Gaussian variance for class c and feature j.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Log probability density.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = MixedNaiveBayes()\n",
    "        >>> model._gaussian_log_pdf(5.0, mean=5.0, var=1.0)\n",
    "        0.0\n",
    "        \"\"\"\n",
    "        return -0.5 * (np.log(2.0 * np.pi * var) + ((x - mean) ** 2) / var)\n",
    "\n",
    "    def _bernoulli_log_likelihood(self, x, p):\n",
    "        \"\"\"\n",
    "        Compute log-likelihood for Bernoulli feature.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : int (0 or 1)\n",
    "            Observed feature value.\n",
    "        p : float\n",
    "            P(X=1 | Y=c), Laplace-smoothed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Log-likelihood under Bernoulli distribution.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = MixedNaiveBayes()\n",
    "        >>> model._bernoulli_log_likelihood(1, p=0.8)\n",
    "        -0.223143551...\n",
    "        \"\"\"\n",
    "        return x * np.log(p) + (1.0 - x) * np.log(1.0 - p)\n",
    "\n",
    "    def _categorical_log_likelihood(self, value, probs_dict):\n",
    "        \"\"\"\n",
    "        Compute log-likelihood of a categorical feature.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        value : hashable\n",
    "            Observed category label.\n",
    "        probs_dict : dict\n",
    "            Mapping category -> probability.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Log-likelihood of observing this category.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> probs = {\"red\": 0.7, \"blue\": 0.3}\n",
    "        >>> model = MixedNaiveBayes()\n",
    "        >>> model._categorical_log_likelihood(\"red\", probs)\n",
    "        -0.35667494...\n",
    "        \"\"\"\n",
    "        return np.log(probs_dict[value])\n",
    "    \n",
    "    def _multinomial_log_likelihood(self, x, probs):\n",
    "        \"\"\"\n",
    "        Log-likelihood for a Multinomial feature.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : int or float\n",
    "            Count value for feature j.\n",
    "        probs : float\n",
    "            Probability P(feature j | class c), Laplace-smoothed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            x * log(probability)\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model._multinomial_log_likelihood(3, probs=0.2)\n",
    "        -4.828...\n",
    "        \"\"\"\n",
    "        return x * np.log(probs)\n",
    "\n",
    "  \n",
    "    # Aggregator for full likelihood per sample/class\n",
    "    def _compute_log_likelihood(self, x_row, class_label):\n",
    "        \"\"\"\n",
    "        Compute sum of log-likelihoods across all features for a class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_row : array-like of shape (n_features,)\n",
    "            Single input sample.\n",
    "        class_label : object\n",
    "            Class for which likelihood is computed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Total log-likelihood for this sample under class c.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model._compute_log_likelihood([\"red\"], \"A\")  # with categorical model\n",
    "        -1.2\n",
    "        \"\"\"\n",
    "        log_l = 0.0\n",
    "\n",
    "        for j, ftype in enumerate(self.feature_types):\n",
    "            value = x_row[j]\n",
    "\n",
    "            if ftype == \"gaussian\":\n",
    "                log_l += self._gaussian_log_pdf(\n",
    "                    x=value,\n",
    "                    mean=self.gaussian_means_[class_label][j],\n",
    "                    var=self.gaussian_vars_[class_label][j]\n",
    "                )\n",
    "\n",
    "            elif ftype == \"bernoulli\":\n",
    "                p = self.bernoulli_probs_[class_label][j]\n",
    "                log_l += self._bernoulli_log_likelihood(value, p)\n",
    "\n",
    "            elif ftype == \"categorical\":\n",
    "                probs = self.categorical_probs_[class_label][j]\n",
    "                log_l += self._categorical_log_likelihood(value, probs)\n",
    "\n",
    "            elif ftype == \"multinomial\":\n",
    "                p = self.multinomial_probs_[class_label][j]\n",
    "                log_l += self._multinomial_log_likelihood(value, p)\n",
    "\n",
    "        return log_l\n",
    "\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y, feature_types):\n",
    "        \"\"\"\n",
    "        Fit Naive Bayes model by estimating:\n",
    "        - class priors P(Y=c)\n",
    "        - Gaussian parameters (mean, variance)\n",
    "        - Bernoulli parameters with Laplace smoothing\n",
    "        - Categorical parameters with Laplace smoothing\n",
    "        - Multinomial parameters with Laplace smoothing\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training feature matrix.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Training class labels.\n",
    "        feature_types : list of str\n",
    "            List specifying type of each feature:\n",
    "            \"gaussian\", \"bernoulli\", \"categorical\", or \"multinomial\".\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : MixedNaiveBayes\n",
    "            Trained model.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = MixedNaiveBayes()\n",
    "        >>> model.fit(X, y, [\"gaussian\", \"bernoulli\"])\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self.feature_types = feature_types\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        multinomial_idx = [j for j, f in enumerate(feature_types) if f == \"multinomial\"]\n",
    "\n",
    "        # record categorical possible values\n",
    "        for j, ftype in enumerate(feature_types):\n",
    "            if ftype == \"categorical\":\n",
    "                self.categorical_values_[j] = np.unique(X[:, j])\n",
    "\n",
    "        # compute class priors WITHOUT smoothing\n",
    "        for c in self.classes_:\n",
    "            Nc = np.sum(y == c)\n",
    "            self.log_priors_[c] = np.log(Nc / n_samples)\n",
    "\n",
    "        # initialize dicts\n",
    "        for c in self.classes_:\n",
    "            self.gaussian_means_[c] = {}\n",
    "            self.gaussian_vars_[c] = {}\n",
    "            self.bernoulli_probs_[c] = {}\n",
    "            self.categorical_probs_[c] = {}\n",
    "            self.multinomial_probs_[c] = {}\n",
    "\n",
    "\n",
    "        # compute likelihood parameters\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y == c]\n",
    "            Nc = len(Xc)\n",
    "\n",
    "            if len(multinomial_idx) > 0:\n",
    "                total_multinomial_count = np.sum(Xc[:, multinomial_idx].astype(float))\n",
    "            else:\n",
    "                total_multinomial_count = 0\n",
    "\n",
    "            for j, ftype in enumerate(feature_types):\n",
    "\n",
    "                # Gaussian MLE\n",
    "                if ftype == \"gaussian\":\n",
    "                    mean = Xc[:, j].mean()\n",
    "                    var = Xc[:, j].var() + self.eps\n",
    "                    self.gaussian_means_[c][j] = mean\n",
    "                    self.gaussian_vars_[c][j] = var\n",
    "\n",
    "                # Bernoulli likelihood with Laplace smoothing\n",
    "                elif ftype == \"bernoulli\":\n",
    "                    count1 = np.sum(Xc[:, j] == 1)\n",
    "                    p = (count1 + self.alpha) / (Nc + 2 * self.alpha)\n",
    "                    self.bernoulli_probs_[c][j] = p\n",
    "\n",
    "                # Categorical likelihood with Laplace smoothing\n",
    "                elif ftype == \"categorical\":\n",
    "                    values = self.categorical_values_[j]\n",
    "                    K = len(values)\n",
    "\n",
    "                    probs = {}\n",
    "                    for v in values:\n",
    "                        count_v = np.sum(Xc[:, j] == v)\n",
    "                        p_v = (count_v + self.alpha) / (Nc + K * self.alpha)\n",
    "                        probs[v] = p_v\n",
    "\n",
    "                    self.categorical_probs_[c][j] = probs\n",
    "\n",
    "                # Multinomial likelihood\n",
    "                elif ftype == \"multinomial\":\n",
    "\n",
    "\n",
    "                    count_j = np.sum(Xc[:, j].astype(float))\n",
    "                    denom = total_multinomial_count + self.alpha * max(len(multinomial_idx), 1)\n",
    "                    p_j = (count_j + self.alpha) / denom\n",
    "                    self.multinomial_probs_[c][j] = p_j\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Prediction\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute log posterior scores for each class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Feature matrix for prediction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_probs : ndarray of shape (n_samples, n_classes)\n",
    "            Raw log posterior scores (not normalized).\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> logp = model.predict_log_proba(X)\n",
    "        >>> logp.shape\n",
    "        (3, 2)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, len(self.classes_)))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_row = X[i]\n",
    "            for k, c in enumerate(self.classes_):\n",
    "                log_l = self._compute_log_likelihood(x_row, c)\n",
    "                log_probs[i, k] = self.log_priors_[c] + log_l\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Convert log posterior scores into normalized probabilities using\n",
    "        the log-sum-exp trick to avoid numerical instability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        probs : ndarray of shape (n_samples, n_classes)\n",
    "            Normalized class probability distribution.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model.predict_proba(X)\n",
    "        array([[0.7, 0.3],\n",
    "               [0.4, 0.6]])\n",
    "        \"\"\"\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "\n",
    "        max_log = np.max(log_probs, axis=1, keepdims=True)\n",
    "        shifted = log_probs - max_log\n",
    "        probs = np.exp(shifted)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the most likely class for each sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        labels : ndarray of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model.predict(X)\n",
    "        array(['A', 'B'], dtype='<U1')\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        class_idx = np.argmax(probs, axis=1)\n",
    "        return self.classes_[class_idx]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
