{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463e43e9",
   "metadata": {},
   "source": [
    "## Overview of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ccc83",
   "metadata": {},
   "source": [
    "Naive Bayes is a family of probabilistic classification methods built on generative modeling, where the goal is to characterize how the features and labels are jointly distributed. Instead of directly learning the conditional probability $P(y \\mid X)$, the method models the joint structure by estimating the class prior distribution together with the class-conditional distributions of the features. The posterior probability used for classification is then obtained via Bayes' rule.\n",
    "\n",
    "At the core of Naive Bayes is the naive conditional independence assumption: given the class label, all features are treated as independent. While this assumption is often violated in real-world data, it greatly simplifies the modeling process, allowing all parameters to be estimated efficiently using simple empirical statistics computed within each class.\n",
    "\n",
    "Different variants of Naive Bayes arise from different assumptions about the class-conditional feature distributions. For example, Multinomial and Bernoulli Naive Bayes are commonly used for text data, while Categorical Naive Bayes applies to discrete, non-numeric features. \n",
    "\n",
    "In this project, we focus on Gaussian Naive Bayes, which assumes that each continuous feature follows a class-conditional normal distribution. This makes GaussianNB particularly suitable for datasets with real-valued features and provides a simple yet effective generative model for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3c857",
   "metadata": {},
   "source": [
    "| Variant | Feature Type | Distribution Assumption | Typical Use Case |\n",
    "|--------|--------------|-------------------------|------------------|\n",
    "| **Gaussian Naive Bayes** | Continuous numerical features | Gaussian (normal) distribution | Sensor data, measurements, general continuous features |\n",
    "| **Multinomial Naive Bayes** | Count-based features | Multinomial distribution | Text classification, word counts (BoW, TF) |\n",
    "| **Bernoulli Naive Bayes** | Binary (0/1) features | Bernoulli distribution | Word presence/absence, binary indicators |\n",
    "| **Categorical Naive Bayes** | General categorical features | Categorical distribution | Discrete categorical variables (color, city, etc.) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdf00a",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616631f1",
   "metadata": {},
   "source": [
    "### **Advantages**\n",
    "\n",
    "**1. Extremely fast training**  \n",
    "GaussianNB uses closed-form estimates for class means and variances, requiring no iterative optimization. \n",
    "\n",
    "**2. Performs well on small datasets**  \n",
    "Estimating Gaussian parameters requires relatively few samples, allowing GNB to work effectively even when data is limited.\n",
    "\n",
    "**3. Simple and interpretable**  \n",
    "The contribution of each feature to the final prediction can be clearly understood through class-wise means, variances, and log-likelihoods.\n",
    "\n",
    "**4. Efficient in high-dimensional spaces**  \n",
    "Due to the independence assumption, the computational cost scales linearly with the number of features, making GNB suitable for high-dimensional data.\n",
    "\n",
    "\n",
    "### **Disadvantages**\n",
    "\n",
    "**1. Independence assumption rarely holds**  \n",
    "The naive assumption that features are conditionally independent is often violated in real-world data, which can degrade model performance.\n",
    "\n",
    "**2. Sensitive to distribution mismatch**  \n",
    "GNB assumes each feature follows a Gaussian distribution within each class. Strongly skewed, multimodal, or heavy-tailed distributions may lead to poor results.\n",
    "\n",
    "**3. Limited model flexibility**  \n",
    "The decision boundaries are quadratic, restricting the model’s ability to capture more complex nonlinear structures.\n",
    "\n",
    "**4. Assumes each feature dimension is an independent univariate Gaussian**  \n",
    "GNB does not capture correlations between features (i.e., it does not model full multivariate Gaussian distributions), which can be limiting when feature interactions are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1563ed",
   "metadata": {},
   "source": [
    "## Representation\n",
    "\n",
    "Gaussian Naive Bayes is a **generative classifier**.  \n",
    "It models how the pair $(x, y)$ is generated, and then uses Bayes’ rule to\n",
    "predict the most likely class for a new feature vector $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Bayes rule\n",
    "\n",
    "We start from Bayes’ rule for a single class $c$:\n",
    "\n",
    "$$\n",
    "P(y = c \\mid x)\n",
    "= \\frac{P(x \\mid y = c)\\, P(y = c)}{P(x)} .\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $P(y = c \\mid x)$: posterior — probability that the label is class $c$ given $x$  \n",
    "- $P(x \\mid y = c)$: likelihood of seeing features $x$ under class $c$  \n",
    "- $P(y = c)$: class prior  \n",
    "- $P(x)$: evidence (normalization term, same for all classes when we compare them)\n",
    "\n",
    "---\n",
    "\n",
    "### Class prior $P(y = c)$\n",
    "\n",
    "We estimate the prior by counting how often each class appears in the training set.\n",
    "Let $N_c$ be the number of samples with label $c$, and $N$ the total number of\n",
    "samples:\n",
    "\n",
    "$$\n",
    "P(y = c) \\approx \\hat P(y = c)\n",
    "= \\frac{N_c}{N} .\n",
    "$$\n",
    "\n",
    "So the prior just measures how common class $c$ is in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood of features $P(x \\mid y = c)$\n",
    "\n",
    "Let the feature vector be $x = (x_1, \\dots, x_d)$.\n",
    "\n",
    "1. **Naive conditional independence**\n",
    "\n",
    "   Given the class $y = c$, features are assumed conditionally independent:\n",
    "\n",
    "   $$\n",
    "   P(x \\mid y = c)\n",
    "   = \\prod_{j=1}^d P(x_j \\mid y = c) .\n",
    "   $$\n",
    "\n",
    "2. **Class-conditional Gaussian distributions**\n",
    "\n",
    "   For each class $c$ and feature $j$, we assume a univariate Gaussian:\n",
    "\n",
    "   $$\n",
    "   x_j \\mid y = c \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2),\n",
    "   $$\n",
    "\n",
    "   so the per-feature likelihood is\n",
    "\n",
    "   $$\n",
    "   P(x_j \\mid y = c)\n",
    "   = \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "   = \\frac{1}{\\sqrt{2\\pi\\sigma_{c,j}^2}}\n",
    "     \\exp\\!\\left(\n",
    "       -\\frac{(x_j - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\n",
    "     \\right).\n",
    "   $$\n",
    "\n",
    "   Combining these, the total likelihood is\n",
    "\n",
    "   $$\n",
    "   P(x \\mid y = c)\n",
    "   = \\prod_{j=1}^d\n",
    "     \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2).\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Final representation (prediction rule)\n",
    "\n",
    "For prediction we choose the class with the largest posterior probability:\n",
    "\n",
    "$$\n",
    "\\hat{y}\n",
    "= \\arg\\max_{c} P(y = c \\mid x).\n",
    "$$\n",
    "\n",
    "Using Bayes’ rule and dropping $P(x)$, which is the same for all classes,\n",
    "this becomes\n",
    "\n",
    "$$\n",
    "\\hat{y}\n",
    "= \\arg\\max_{c} P(x \\mid y = c)\\, P(y = c)\n",
    "= \\arg\\max_{c}\n",
    "\\left[\n",
    "  \\prod_{j=1}^d\n",
    "  \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "\\right]\n",
    "\\frac{N_c}{N}.\n",
    "$$\n",
    "\n",
    "In code, we usually work in **log space**, replacing products by sums:\n",
    "\n",
    "$$\n",
    "\\log P(y = c \\mid x)\n",
    "= \\log P(y = c)\n",
    "+ \\sum_{j=1}^d \\log \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "\\quad \\text{(up to an additive constant)} ,\n",
    "$$\n",
    "\n",
    "and still predict\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{c} \\log P(y = c \\mid x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351404c",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "We train Gaussian Naive Bayes by **maximum likelihood**,  \n",
    "which is equivalent to **minimizing the negative log-likelihood (NLL)**.\n",
    "\n",
    "---\n",
    "\n",
    "### General form\n",
    "\n",
    "Given a parametric model with parameters $\\Theta$ and training data\n",
    "$\\{(x_i, y_i)\\}_{i=1}^n$, the likelihood of the data is\n",
    "\n",
    "$$\n",
    "p_\\Theta(\\{x_i, y_i\\}_{i=1}^n)\n",
    "= \\prod_{i=1}^n p_\\Theta(y_i, x_i).\n",
    "$$\n",
    "\n",
    "The **negative log-likelihood loss** is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\Theta)\n",
    "= - \\sum_{i=1}^n \\log p_\\Theta(y_i, x_i).\n",
    "$$\n",
    "\n",
    "Minimizing $\\mathcal{L}_{\\text{NLL}}$ is the same as maximizing the likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "### NLL for Gaussian Naive Bayes\n",
    "\n",
    "For Gaussian NB, the joint model for a single sample is\n",
    "\n",
    "$$\n",
    "p_\\Theta(x_i, y_i)\n",
    "= \\pi_{y_i}\n",
    "  \\prod_{j=1}^d\n",
    "  \\mathcal{N}(x_{i,j} \\mid \\mu_{y_i,j}, \\sigma_{y_i,j}^2),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\pi_c = P(y = c)$ is the class prior,  \n",
    "- $\\mu_{c,j}$ and $\\sigma_{c,j}^2$ are the mean and variance of feature $j$ in class $c$.\n",
    "\n",
    "Plugging this into the general NLL and expanding the Gaussian log-density gives\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\Theta)\n",
    "=\n",
    "\\sum_{i=1}^n\n",
    "\\left[\n",
    "-\\log \\pi_{y_i}\n",
    "+\n",
    "\\sum_{j=1}^d\n",
    "\\left(\n",
    "\\frac{1}{2}\\log\\!\\big(2\\pi \\sigma_{y_i,j}^2\\big)\n",
    "+\n",
    "\\frac{(x_{i,j} - \\mu_{y_i,j})^2}{2\\sigma_{y_i,j}^2}\n",
    "\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "- The term $-\\log \\pi_{y_i}$ comes from the **class prior**.  \n",
    "- The inner sum over $j$ comes from the **Gaussian likelihood** of each feature.\n",
    "\n",
    "Because Gaussian NB has **closed-form MLE solutions** for $\\pi_c$, $\\mu_{c,j}$, and\n",
    "$\\sigma_{c,j}^2$, we usually do **not** run gradient descent on this loss in practice;\n",
    "instead we compute the empirical counts, means, and variances that minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99866d2a",
   "metadata": {},
   "source": [
    "## Optimizer(Not Real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830bf7a",
   "metadata": {},
   "source": [
    "Unlike discriminative models such as logistic regression or neural networks,  \n",
    "Gaussian Naive Bayes does not require an iterative optimizer.  \n",
    "Although the model minimizes the negative log-likelihood (NLL), the optimal parameters\n",
    "have closed-form maximum likelihood solutions.\n",
    "\n",
    "For each class $c$ and feature $j$, the MLE updates are:\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_c = \\frac{N_c}{N},\n",
    "$$\n",
    "where $N_c$ is the number of samples in class $c$.\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{c,j}\n",
    "= \\frac{1}{N_c} \\sum_{i : y_i = c} x_{i,j}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{c,j}^2\n",
    "= \\frac{1}{N_c} \\sum_{i : y_i = c} (x_{i,j} - \\hat{\\mu}_{c,j})^2.\n",
    "$$\n",
    "\n",
    "These formulas directly minimize the NLL loss, so training requires only computing\n",
    "class counts, sample means, and sample variances—no gradient descent, no iterative\n",
    "optimization, and no numerical solver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e4845",
   "metadata": {},
   "source": [
    "## Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4497a",
   "metadata": {},
   "source": [
    "Training Gaussian Naive Bayes model  \n",
    "**Require:** Training dataset $(X, y)$, smoothing parameter $\\text{var\\_smoothing}$  \n",
    "**Ensure:** Class priors $P(c)$, means $\\mu_{c,j}$, variances $\\sigma_{c,j}^2$  \n",
    "\n",
    "1:  $N \\leftarrow$ number of samples in $X$  \n",
    "2:  **for** $j = 1 \\dots d$ **do**  \n",
    "3:  $\\hat{\\sigma}_j^2 \\leftarrow \\frac{1}{N} \\sum_{i=1}^N (X_{i,j} - \\bar{X}_j)^2$  \n",
    "4:  **end for**  \n",
    "5:  $v_{\\max} \\leftarrow \\max_{1 \\le j \\le d} \\hat{\\sigma}_j^2$  \n",
    "6:  $\\epsilon \\leftarrow \\text{var\\_smoothing} \\cdot v_{\\max}$  \n",
    "\n",
    "7:  $C \\leftarrow \\text{unique}(y)$  \n",
    "8:  **for each** class $c \\in C$ **do**  \n",
    "9:   $S_c \\leftarrow \\{\\, i : y_i = c \\,\\}$  \n",
    "10:   $N_c \\leftarrow |S_c|$  \n",
    "11:   $P(c) \\leftarrow \\dfrac{N_c}{N}$  \n",
    "12:   **for** $j = 1 \\dots d$ **do**  \n",
    "13:    $X_{c,j} \\leftarrow \\{\\, X_{i,j} : i \\in S_c \\,\\}$  \n",
    "14:    $\\mu_{c,j} \\leftarrow \\dfrac{1}{N_c} \\sum_{i \\in S_c} X_{i,j}$  \n",
    "15:    $\\sigma_{c,j}^2 \\leftarrow \\dfrac{1}{N_c} \\sum_{i \\in S_c} (X_{i,j} - \\mu_{c,j})^2 + \\epsilon$  \n",
    "16:   **end for**  \n",
    "17: **end for**  \n",
    "\n",
    "---\n",
    "\n",
    "Predicting with Gaussian Naive Bayes  \n",
    "**Require:** Model parameters $P(c)$, $\\mu_{c,j}$, $\\sigma_{c,j}^2$  \n",
    "**Ensure:** Predicted label $\\hat{y}$  \n",
    "\n",
    "1:  **for each** class $c \\in C$ **do**  \n",
    "2:  $\\text{score}(c) \\leftarrow \\log P(c)$  \n",
    "3:  **for** $j = 1 \\dots d$ **do**  \n",
    "4:   $\\ell \\leftarrow -\\frac{1}{2}\\log(2\\pi\\sigma_{c,j}^2) \n",
    "      \\;-\\; \\frac{(x_j - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}$  \n",
    "5:   $\\text{score}(c) \\leftarrow \\text{score}(c) + \\ell$  \n",
    "6:  **end for**  \n",
    "7: **end for**  \n",
    "8:  $\\hat{y} \\leftarrow \\arg\\max_{c} \\text{score}(c)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
