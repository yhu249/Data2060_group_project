{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899ca05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MixedNaiveBayes:\n",
    "    \"\"\"\n",
    "    A Naive Bayes classifier that can handle three different feature types:\n",
    "    gaussian for continuous values represented by a normal distribution\n",
    "    bernoulli for binary values with Laplace smoothing\n",
    "    categorical for discrete string or integer values with Laplace smoothing\n",
    "\n",
    "    All probability calculations are done in log space. This avoids numerical\n",
    "    underflow when probabilities become very small. During prediction the model\n",
    "    converts log probabilities back into normal probabilities and normalizes\n",
    "    them so that each row sums to one.\n",
    "\n",
    "    Feature likelihoods\n",
    "    -------------------\n",
    "    For gaussian features:\n",
    "        log_pdf = -0.5 * ( log(2 * pi * variance) + ((x - mean)^2) / variance )\n",
    "\n",
    "    For bernoulli features:\n",
    "        p1 = (count_of_1_in_class + alpha) / (n_class + 2 * alpha)\n",
    "        log_likelihood = x * log(p1) + (1 - x) * log(1 - p1)\n",
    "\n",
    "    For categorical features:\n",
    "        p(value) = (count_of_value_in_class + alpha) / (n_class + K * alpha)\n",
    "        log_likelihood = log( p(value) )\n",
    "        Here K is the number of possible categories for that feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eps : float\n",
    "        Small constant added to Gaussian variances for stability.\n",
    "    laplace_alpha : float\n",
    "        Laplace smoothing parameter used for Bernoulli and categorical features.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This model assumes that categorical test values always appear in the\n",
    "    training data. Any preprocessing or encoding needed to align categories\n",
    "    must be done before calling fit.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> X = np.array([[5.1, 1, \"red\"],\n",
    "                      [4.9, 0, \"blue\"],\n",
    "                      [5.0, 1, \"red\"]])\n",
    "    >>> y = np.array([\"A\", \"B\", \"A\"])\n",
    "    >>> feature_types = [\"gaussian\", \"bernoulli\", \"categorical\"]\n",
    "    >>> model = MixedNaiveBayes()\n",
    "    >>> model.fit(X, y, feature_types)\n",
    "    >>> model.predict(X)\n",
    "    array(['A', 'B', 'A'], dtype='<U1')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-8, laplace_alpha=1.0):\n",
    "        self.eps = eps\n",
    "        self.alpha = laplace_alpha\n",
    "\n",
    "        self.feature_types = None\n",
    "        self.classes_ = None\n",
    "\n",
    "        self.log_priors_ = {}\n",
    "\n",
    "        self.gaussian_means_ = {}\n",
    "        self.gaussian_vars_ = {}\n",
    "        self.bernoulli_probs_ = {}\n",
    "        self.categorical_probs_ = {}\n",
    "        self.categorical_values_ = {}\n",
    "\n",
    "    # Likelihood helper functions\n",
    "\n",
    "    def _gaussian_log_pdf(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Computes the log density of a Gaussian distribution for a single feature.\n",
    "        This function is for continuous values.\n",
    "        \"\"\"\n",
    "        return -0.5 * (np.log(2.0 * np.pi * var) + ((x - mean) ** 2) / var)\n",
    "\n",
    "    def _bernoulli_log_likelihood(self, x, p):\n",
    "        \"\"\"\n",
    "        Computes the log likelihood of a Bernoulli feature.\n",
    "        x must be 0 or 1 and p is the probability of seeing a 1.\n",
    "        \"\"\"\n",
    "        return x * np.log(p) + (1.0 - x) * np.log(1.0 - p)\n",
    "\n",
    "    def _categorical_log_likelihood(self, value, probs_dict):\n",
    "        \"\"\"\n",
    "        Computes the log likelihood of a categorical feature.\n",
    "        The feature value must exist in the dictionary created during training.\n",
    "        \"\"\"\n",
    "        return np.log(probs_dict[value])\n",
    "\n",
    "    # Aggregator\n",
    "    def _compute_log_likelihood(self, x_row, class_label):\n",
    "        \"\"\"\n",
    "        Computes the total log likelihood of a sample under a specific class.\n",
    "        This is the sum of the log likelihoods of each feature.\n",
    "        \"\"\"\n",
    "        log_l = 0.0\n",
    "\n",
    "        for j, ftype in enumerate(self.feature_types):\n",
    "            value = x_row[j]\n",
    "\n",
    "            if ftype == \"gaussian\":\n",
    "                mean = self.gaussian_means_[class_label][j]\n",
    "                var = self.gaussian_vars_[class_label][j]\n",
    "                log_l += self._gaussian_log_pdf(value, mean, var)\n",
    "\n",
    "            elif ftype == \"bernoulli\":\n",
    "                p = self.bernoulli_probs_[class_label][j]\n",
    "                log_l += self._bernoulli_log_likelihood(value, p)\n",
    "\n",
    "            elif ftype == \"categorical\":\n",
    "                probs_dict = self.categorical_probs_[class_label][j]\n",
    "                log_l += self._categorical_log_likelihood(value, probs_dict)\n",
    "\n",
    "        return log_l\n",
    "    # Fit\n",
    "\n",
    "    def fit(self, X, y, feature_types):\n",
    "        \"\"\"\n",
    "        Learns all model parameters. This includes class priors as well as the\n",
    "        parameters needed for each feature type.\n",
    "\n",
    "        Inputs\n",
    "        ------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Class labels.\n",
    "        feature_types : list of strings\n",
    "            One entry per feature indicating whether it is gaussian,\n",
    "            bernoulli or categorical.\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        Returns the trained model instance.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        self.feature_types = feature_types\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        for j, ftype in enumerate(feature_types):\n",
    "            if ftype == \"categorical\":\n",
    "                self.categorical_values_[j] = np.unique(X[:, j])\n",
    "\n",
    "        for c in self.classes_:\n",
    "            n_c = np.sum(y == c)\n",
    "            self.log_priors_[c] = np.log(n_c / n_samples)\n",
    "\n",
    "        for c in self.classes_:\n",
    "            self.gaussian_means_[c] = {}\n",
    "            self.gaussian_vars_[c] = {}\n",
    "            self.bernoulli_probs_[c] = {}\n",
    "            self.categorical_probs_[c] = {}\n",
    "\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y == c]\n",
    "            n_c = len(Xc)\n",
    "\n",
    "            for j, ftype in enumerate(feature_types):\n",
    "\n",
    "                if ftype == \"gaussian\":\n",
    "                    mean = Xc[:, j].mean()\n",
    "                    var = Xc[:, j].var() + self.eps\n",
    "                    self.gaussian_means_[c][j] = mean\n",
    "                    self.gaussian_vars_[c][j] = var\n",
    "\n",
    "                elif ftype == \"bernoulli\":\n",
    "                    count1 = np.sum(Xc[:, j] == 1)\n",
    "                    K = 2\n",
    "                    p1 = (count1 + self.alpha) / (n_c + K * self.alpha)\n",
    "                    self.bernoulli_probs_[c][j] = p1\n",
    "\n",
    "                elif ftype == \"categorical\":\n",
    "                    values = self.categorical_values_[j]\n",
    "                    K = len(values)\n",
    "\n",
    "                    probs = {}\n",
    "                    for v in values:\n",
    "                        count_v = np.sum(Xc[:, j] == v)\n",
    "                        p_v = (count_v + self.alpha) / (n_c + K * self.alpha)\n",
    "                        probs[v] = p_v\n",
    "\n",
    "                    self.categorical_probs_[c][j] = probs\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Prediction\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Computes unnormalized log posterior scores for each class.\n",
    "        These are the raw log scores before normalization.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, len(self.classes_)))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_row = X[i]\n",
    "            for k, c in enumerate(self.classes_):\n",
    "                log_l = self._compute_log_likelihood(x_row, c)\n",
    "                log_probs[i, k] = self.log_priors_[c] + log_l\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Converts log posterior scores into normalized probabilities.\n",
    "        Uses the log-sum-exp trick for numerical stability.\n",
    "\n",
    "        Each row of the output represents the probability distribution\n",
    "        over all classes for that sample.\n",
    "        \"\"\"\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "\n",
    "        max_log = np.max(log_probs, axis=1, keepdims=True)\n",
    "        shifted = log_probs - max_log\n",
    "\n",
    "        probs = np.exp(shifted)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Returns the predicted class label for each sample.\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        class_indices = np.argmax(probs, axis=1)\n",
    "        return self.classes_[class_indices]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
