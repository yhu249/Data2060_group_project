{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463e43e9",
   "metadata": {},
   "source": [
    "## Overview of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ccc83",
   "metadata": {},
   "source": [
    "Naive Bayes is a family of probabilistic classification methods built on generative modeling, where the goal is to characterize how the features and labels are jointly distributed. Instead of directly learning the conditional probability $P(y \\mid X)$, the method models the joint structure by estimating the class prior distribution together with the class-conditional distributions of the features. The posterior probability used for classification is then obtained via Bayes' rule.\n",
    "\n",
    "At the core of Naive Bayes is the naive conditional independence assumption: given the class label, all features are treated as independent. While this assumption is often violated in real-world data, it greatly simplifies the modeling process, allowing all parameters to be estimated efficiently using simple empirical statistics computed within each class.\n",
    "\n",
    "Different variants of Naive Bayes arise from different assumptions about the class-conditional feature distributions. For example, Multinomial and Bernoulli Naive Bayes are commonly used for text data, while Categorical Naive Bayes applies to discrete, non-numeric features. \n",
    "\n",
    "In this project, we focus on Gaussian Naive Bayes, which assumes that each continuous feature follows a class-conditional normal distribution. This makes GaussianNB particularly suitable for datasets with real-valued features and provides a simple yet effective generative model for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3c857",
   "metadata": {},
   "source": [
    "| Variant | Feature Type | Distribution Assumption | Typical Use Case |\n",
    "|--------|--------------|-------------------------|------------------|\n",
    "| **Gaussian Naive Bayes** | Continuous numerical features | Gaussian (normal) distribution | Sensor data, measurements, general continuous features |\n",
    "| **Multinomial Naive Bayes** | Count-based features | Multinomial distribution | Text classification, word counts (BoW, TF) |\n",
    "| **Bernoulli Naive Bayes** | Binary (0/1) features | Bernoulli distribution | Word presence/absence, binary indicators |\n",
    "| **Categorical Naive Bayes** | General categorical features | Categorical distribution | Discrete categorical variables (color, city, etc.) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdf00a",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616631f1",
   "metadata": {},
   "source": [
    "### **Advantages**\n",
    "\n",
    "**1. Extremely fast training**  \n",
    "GaussianNB uses closed-form estimates for class means and variances, requiring no iterative optimization. \n",
    "\n",
    "**2. Performs well on small datasets**  \n",
    "Estimating Gaussian parameters requires relatively few samples, allowing GNB to work effectively even when data is limited.\n",
    "\n",
    "**3. Simple and interpretable**  \n",
    "The contribution of each feature to the final prediction can be clearly understood through class-wise means, variances, and log-likelihoods.\n",
    "\n",
    "**4. Efficient in high-dimensional spaces**  \n",
    "Due to the independence assumption, the computational cost scales linearly with the number of features, making GNB suitable for high-dimensional data.\n",
    "\n",
    "\n",
    "### **Disadvantages**\n",
    "\n",
    "**1. Independence assumption rarely holds**  \n",
    "The naive assumption that features are conditionally independent is often violated in real-world data, which can degrade model performance.\n",
    "\n",
    "**2. Sensitive to distribution mismatch**  \n",
    "GNB assumes each feature follows a Gaussian distribution within each class. Strongly skewed, multimodal, or heavy-tailed distributions may lead to poor results.\n",
    "\n",
    "**3. Limited model flexibility**  \n",
    "The decision boundaries are quadratic, restricting the model’s ability to capture more complex nonlinear structures.\n",
    "\n",
    "**4. Assumes each feature dimension is an independent univariate Gaussian**  \n",
    "GNB does not capture correlations between features (i.e., it does not model full multivariate Gaussian distributions), which can be limiting when feature interactions are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c64f7",
   "metadata": {},
   "source": [
    "## Problem Assumptions\n",
    "\n",
    "Gaussian Naive Bayes is a **generative classifier**:  \n",
    "it models the joint distribution \\(p(x, y)\\) and then uses Bayes’ rule\n",
    "to predict the most likely class \\(y\\) for a new feature vector \\(x\\).\n",
    "\n",
    "---\n",
    "\n",
    "### Notation\n",
    "\n",
    "- $x \\in \\mathbb{R}^d$: feature vector with $d$ continuous attributes  \n",
    "- $y \\in \\{c_1, \\dots, c_K\\}$: class label (categorical)  \n",
    "- $\\mu_{c,j}, \\sigma_{c,j}^2$: mean and variance of feature $j$ in class $c$\n",
    "\n",
    "---\n",
    "\n",
    "### Assumption 1 — Class-conditional Gaussian distributions\n",
    "\n",
    "For each class \\(c\\) and each feature \\(j\\), the feature value follows\n",
    "a **Gaussian distribution** within that class:\n",
    "\n",
    "$$\n",
    "x_j \\mid (y = c) \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2).\n",
    "$$\n",
    "\n",
    "So, conditioned on the class, each feature is modeled\n",
    "as a one-dimensional normal variable with its own mean and variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumption 2 — Naive conditional independence & generative model\n",
    "\n",
    "Given the class label \\(y = c\\), all features are assumed to be\n",
    "**conditionally independent**:\n",
    "\n",
    "$$\n",
    "p(x \\mid y = c)\n",
    "= \\prod_{j=1}^d p(x_j \\mid y = c).\n",
    "$$\n",
    "\n",
    "Using this together with the class prior \\(p(y = c)\\),\n",
    "the **generative joint model** factorizes as\n",
    "\n",
    "$$\n",
    "p(x, y = c)\n",
    "= p(y = c)\\, p(x \\mid y = c)\n",
    "= \\pi_c \\prod_{j=1}^d \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2),\n",
    "$$\n",
    "\n",
    "where \\(\\pi_c = p(y = c)\\) is the class prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1563ed",
   "metadata": {},
   "source": [
    "## Representation\n",
    "\n",
    "Gaussian Naive Bayes is a **generative classifier**.  \n",
    "It models how the pair $(x, y)$ is generated, and then uses Bayes’ rule to\n",
    "predict the most likely class for a new feature vector $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Bayes rule\n",
    "\n",
    "We start from Bayes’ rule for a single class $c$:\n",
    "\n",
    "$$\n",
    "P(y = c \\mid x)\n",
    "= \\frac{P(x \\mid y = c)\\, P(y = c)}{P(x)} .\n",
    "$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $P(y = c \\mid x)$: posterior — probability that the label is class $c$ given $x$  \n",
    "- $P(x \\mid y = c)$: likelihood of seeing features $x$ under class $c$  \n",
    "- $P(y = c)$: class prior  \n",
    "- $P(x)$: evidence (normalization term, same for all classes when we compare them)\n",
    "\n",
    "---\n",
    "\n",
    "### Class prior $P(y = c)$\n",
    "\n",
    "We estimate the prior by counting how often each class appears in the training set.\n",
    "Let $N_c$ be the number of samples with label $c$, and $N$ the total number of\n",
    "samples:\n",
    "\n",
    "$$\n",
    "P(y = c) \\approx \\hat P(y = c)\n",
    "= \\frac{N_c}{N} .\n",
    "$$\n",
    "\n",
    "So the prior just measures how common class $c$ is in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood of features $P(x \\mid y = c)$\n",
    "\n",
    "Let the feature vector be $x = (x_1, \\dots, x_d)$.\n",
    "\n",
    "1. **Naive conditional independence**\n",
    "\n",
    "   Given the class $y = c$, features are assumed conditionally independent:\n",
    "\n",
    "   $$\n",
    "   P(x \\mid y = c)\n",
    "   = \\prod_{j=1}^d P(x_j \\mid y = c) .\n",
    "   $$\n",
    "\n",
    "2. **Class-conditional Gaussian distributions**\n",
    "\n",
    "   For each class $c$ and feature $j$, we assume a univariate Gaussian:\n",
    "\n",
    "   $$\n",
    "   x_j \\mid y = c \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2),\n",
    "   $$\n",
    "\n",
    "   so the per-feature likelihood is\n",
    "\n",
    "   $$\n",
    "   P(x_j \\mid y = c)\n",
    "   = \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "   = \\frac{1}{\\sqrt{2\\pi\\sigma_{c,j}^2}}\n",
    "     \\exp\\!\\left(\n",
    "       -\\frac{(x_j - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\n",
    "     \\right).\n",
    "   $$\n",
    "\n",
    "   Combining these, the total likelihood is\n",
    "\n",
    "   $$\n",
    "   P(x \\mid y = c)\n",
    "   = \\prod_{j=1}^d\n",
    "     \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2).\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Final representation (prediction rule)\n",
    "\n",
    "For prediction we choose the class with the largest posterior probability:\n",
    "\n",
    "$$\n",
    "\\hat{y}\n",
    "= \\arg\\max_{c} P(y = c \\mid x).\n",
    "$$\n",
    "\n",
    "Using Bayes’ rule and dropping $P(x)$, which is the same for all classes,\n",
    "this becomes\n",
    "\n",
    "$$\n",
    "\\hat{y}\n",
    "= \\arg\\max_{c} P(x \\mid y = c)\\, P(y = c)\n",
    "= \\arg\\max_{c}\n",
    "\\left[\n",
    "  \\prod_{j=1}^d\n",
    "  \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "\\right]\n",
    "\\frac{N_c}{N}.\n",
    "$$\n",
    "\n",
    "In code, we usually work in **log space**, replacing products by sums:\n",
    "\n",
    "$$\n",
    "\\log P(y = c \\mid x)\n",
    "= \\log P(y = c)\n",
    "+ \\sum_{j=1}^d \\log \\mathcal{N}(x_j \\mid \\mu_{c,j}, \\sigma_{c,j}^2)\n",
    "\\quad \\text{(up to an additive constant)} ,\n",
    "$$\n",
    "\n",
    "and still predict\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{c} \\log P(y = c \\mid x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2351404c",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "We train Gaussian Naive Bayes by **maximum likelihood**,  \n",
    "which is equivalent to **minimizing the negative log-likelihood (NLL)**.\n",
    "\n",
    "---\n",
    "\n",
    "### General form\n",
    "\n",
    "Given a parametric model with parameters $\\Theta$ and training data\n",
    "$\\{(x_i, y_i)\\}_{i=1}^n$, the likelihood of the data is\n",
    "\n",
    "$$\n",
    "p_\\Theta(\\{x_i, y_i\\}_{i=1}^n)\n",
    "= \\prod_{i=1}^n p_\\Theta(y_i, x_i).\n",
    "$$\n",
    "\n",
    "The **negative log-likelihood loss** is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\Theta)\n",
    "= - \\sum_{i=1}^n \\log p_\\Theta(y_i, x_i).\n",
    "$$\n",
    "\n",
    "Minimizing $\\mathcal{L}_{\\text{NLL}}$ is the same as maximizing the likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "### NLL for Gaussian Naive Bayes\n",
    "\n",
    "For Gaussian NB, the joint model for a single sample is\n",
    "\n",
    "$$\n",
    "p_\\Theta(x_i, y_i)\n",
    "= \\pi_{y_i}\n",
    "  \\prod_{j=1}^d\n",
    "  \\mathcal{N}(x_{i,j} \\mid \\mu_{y_i,j}, \\sigma_{y_i,j}^2),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\pi_c = P(y = c)$ is the class prior,  \n",
    "- $\\mu_{c,j}$ and $\\sigma_{c,j}^2$ are the mean and variance of feature $j$ in class $c$.\n",
    "\n",
    "Plugging this into the general NLL and expanding the Gaussian log-density gives\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\Theta)\n",
    "=\n",
    "\\sum_{i=1}^n\n",
    "\\left[\n",
    "-\\log \\pi_{y_i}\n",
    "+\n",
    "\\sum_{j=1}^d\n",
    "\\left(\n",
    "\\frac{1}{2}\\log\\!\\big(2\\pi \\sigma_{y_i,j}^2\\big)\n",
    "+\n",
    "\\frac{(x_{i,j} - \\mu_{y_i,j})^2}{2\\sigma_{y_i,j}^2}\n",
    "\\right)\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "- The term $-\\log \\pi_{y_i}$ comes from the **class prior**.  \n",
    "- The inner sum over $j$ comes from the **Gaussian likelihood** of each feature.\n",
    "\n",
    "Because Gaussian NB has **closed-form MLE solutions** for $\\pi_c$, $\\mu_{c,j}$, and\n",
    "$\\sigma_{c,j}^2$, we usually do **not** run gradient descent on this loss in practice;\n",
    "instead we compute the empirical counts, means, and variances that minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99866d2a",
   "metadata": {},
   "source": [
    "## Optimizer(Not Real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830bf7a",
   "metadata": {},
   "source": [
    "Unlike discriminative models such as logistic regression or neural networks,  \n",
    "Gaussian Naive Bayes does not require an iterative optimizer.  \n",
    "Although the model minimizes the negative log-likelihood (NLL), the optimal parameters\n",
    "have closed-form maximum likelihood solutions.\n",
    "\n",
    "For each class $c$ and feature $j$, the MLE updates are:\n",
    "\n",
    "$$\n",
    "\\hat{\\pi}_c = \\frac{N_c}{N},\n",
    "$$\n",
    "where $N_c$ is the number of samples in class $c$.\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_{c,j}\n",
    "= \\frac{1}{N_c} \\sum_{i : y_i = c} x_{i,j}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{c,j}^2\n",
    "= \\frac{1}{N_c} \\sum_{i : y_i = c} (x_{i,j} - \\hat{\\mu}_{c,j})^2.\n",
    "$$\n",
    "\n",
    "These formulas directly minimize the NLL loss, so training requires only computing\n",
    "class counts, sample means, and sample variances—no gradient descent, no iterative\n",
    "optimization, and no numerical solver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e4845",
   "metadata": {},
   "source": [
    "## Pseudo-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4497a",
   "metadata": {},
   "source": [
    "Training Gaussian Naive Bayes model  \n",
    "**Require:** Training dataset $(X, y)$, smoothing parameter $\\text{var\\_smoothing}$  \n",
    "**Ensure:** Class priors $P(c)$, means $\\mu_{c,j}$, variances $\\sigma_{c,j}^2$  \n",
    "\n",
    "1:  $N \\leftarrow$ number of samples in $X$  \n",
    "2:  **for** $j = 1 \\dots d$ **do**  \n",
    "3:  $\\hat{\\sigma}_j^2 \\leftarrow \\frac{1}{N} \\sum_{i=1}^N (X_{i,j} - \\bar{X}_j)^2$  \n",
    "4:  **end for**  \n",
    "5:  $v_{\\max} \\leftarrow \\max_{1 \\le j \\le d} \\hat{\\sigma}_j^2$  \n",
    "6:  $\\epsilon \\leftarrow \\text{var\\_smoothing} \\cdot v_{\\max}$  \n",
    "\n",
    "7:  $C \\leftarrow \\text{unique}(y)$  \n",
    "8:  **for each** class $c \\in C$ **do**  \n",
    "9:   $S_c \\leftarrow \\{\\, i : y_i = c \\,\\}$  \n",
    "10:   $N_c \\leftarrow |S_c|$  \n",
    "11:   $P(c) \\leftarrow \\dfrac{N_c}{N}$  \n",
    "12:   **for** $j = 1 \\dots d$ **do**  \n",
    "13:    $X_{c,j} \\leftarrow \\{\\, X_{i,j} : i \\in S_c \\,\\}$  \n",
    "14:    $\\mu_{c,j} \\leftarrow \\dfrac{1}{N_c} \\sum_{i \\in S_c} X_{i,j}$  \n",
    "15:    $\\sigma_{c,j}^2 \\leftarrow \\dfrac{1}{N_c} \\sum_{i \\in S_c} (X_{i,j} - \\mu_{c,j})^2 + \\epsilon$  \n",
    "16:   **end for**  \n",
    "17: **end for**  \n",
    "\n",
    "---\n",
    "\n",
    "Predicting with Gaussian Naive Bayes  \n",
    "**Require:** Model parameters $P(c)$, $\\mu_{c,j}$, $\\sigma_{c,j}^2$  \n",
    "**Ensure:** Predicted label $\\hat{y}$  \n",
    "\n",
    "1:  **for each** class $c \\in C$ **do**  \n",
    "2:  $\\text{score}(c) \\leftarrow \\log P(c)$  \n",
    "3:  **for** $j = 1 \\dots d$ **do**  \n",
    "4:   $\\ell \\leftarrow -\\frac{1}{2}\\log(2\\pi\\sigma_{c,j}^2) \n",
    "      \\;-\\; \\frac{(x_j - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}$  \n",
    "5:   $\\text{score}(c) \\leftarrow \\text{score}(c) + \\ell$  \n",
    "6:  **end for**  \n",
    "7: **end for**  \n",
    "8:  $\\hat{y} \\leftarrow \\arg\\max_{c} \\text{score}(c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252472ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier.\n",
    "\n",
    "    This classifier assumes all features follow a Gaussian distribution and\n",
    "    estimates per-class means and variances. Variance smoothing is applied\n",
    "    using the formula: epsilon = var_smoothing * max(feature_variances)\n",
    "\n",
    "    All probability calculations are done in log space to avoid numerical\n",
    "    underflow when probabilities become very small. During prediction the model\n",
    "    converts log probabilities back into normal probabilities and normalizes\n",
    "    them so that each row sums to one.\n",
    "\n",
    "    Feature likelihoods\n",
    "    ----------\n",
    "    For Gaussian numeric features:\n",
    "        log_pdf = -0.5 * ( log(2 * pi * variance) + ((x - mean)^2) / variance )\n",
    "\n",
    "    The model uses maximum likelihood estimates of the mean and variance\n",
    "    for each feature within each class, with an additional variance smoothing\n",
    "    term added for numerical stability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    var_smoothing : float, optional (default=1e-9)\n",
    "        Portion of the largest feature variance added to each variance estimate\n",
    "        for numerical stability.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> X = np.array([[1.0, 2.0],\n",
    "    ...               [1.2, 1.9],\n",
    "    ...               [3.2, 4.8],\n",
    "    ...               [3.0, 5.1]])\n",
    "    >>> y = np.array(['A', 'A', 'B', 'B'])\n",
    "    >>> model = GaussianNaiveBayes()\n",
    "    >>> model.fit(X, y)\n",
    "    >>> model.predict([[1.1, 2.0]])\n",
    "    array(['A'], dtype='<U1')\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        \"\"\"\n",
    "        Initialize a Gaussian Naive Bayes classifier.\n",
    "\n",
    "        This constructor sets up the model hyperparameters and internal\n",
    "        data structures that will be populated during fitting.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        var_smoothing : float, default=1e-9\n",
    "            Non-negative smoothing factor added to feature variances\n",
    "            for numerical stability. The actual smoothing term is\n",
    "            scaled by the maximum variance across all features.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        classes_ : ndarray of shape (n_classes,)\n",
    "            Array of unique class labels observed during fitting.\n",
    "\n",
    "        log_priors_ : dict\n",
    "            Dictionary mapping each class label to its log prior\n",
    "            probability log P(Y = c).\n",
    "\n",
    "        gaussian_means_ : dict\n",
    "            Nested dictionary storing per-class per-feature Gaussian means.\n",
    "            gaussian_means_[c][j] = mean of feature j for class c.\n",
    "\n",
    "        gaussian_vars_ : dict\n",
    "            Nested dictionary storing per-class per-feature Gaussian variances\n",
    "            (after variance smoothing).\n",
    "            gaussian_vars_[c][j] = variance of feature j for class c.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - All attributes except var_smoothing are initialized empty and\n",
    "        filled during the call to fit().\n",
    "        \"\"\"\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.log_priors_ = {}\n",
    "        self.gaussian_means_ = {}\n",
    "        self.gaussian_vars_ = {}\n",
    "\n",
    "    # Likelihood helper functions\n",
    "    def _gaussian_log_pdf(self, x, mean, var):\n",
    "        \"\"\"\n",
    "        Compute the log probability density of a Gaussian distribution.\n",
    "\n",
    "        This function evaluates the log of the Gaussian (normal) probability\n",
    "        density function for a single feature value.\n",
    "\n",
    "        The Gaussian log-PDF is given by:\n",
    "            log N(x | mean, var)\n",
    "            = -0.5 * [ log(2π · var) + (x - mean)^2 / var ]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float\n",
    "            Observed feature value.\n",
    "\n",
    "        mean : float\n",
    "            Mean of the Gaussian distribution for the given class and feature.\n",
    "\n",
    "        var : float\n",
    "            Variance of the Gaussian distribution for the given class and feature.\n",
    "            Assumed to be strictly positive after variance smoothing.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Log probability density log P(x | mean, var).\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - Computation is done entirely in log space for numerical stability.\n",
    "        - This method is used as a building block for computing class-conditional\n",
    "        log-likelihoods.\n",
    "        \"\"\"\n",
    "        return -0.5 * (np.log(2.0 * np.pi * var) + ((x - mean) ** 2) / var)\n",
    "\n",
    "    # Aggregator\n",
    "    def _compute_log_likelihood(self, x_row, class_label):\n",
    "        \"\"\"\n",
    "        Compute the total log-likelihood log p(x | c) of a single sample under\n",
    "        a given class using the Gaussian Naive Bayes assumption.\n",
    "\n",
    "        Under the Naive Bayes conditional independence (i.i.d.) assumption,\n",
    "        the joint likelihood factorizes across features:\n",
    "            p(x | c) = ∏_j p(x_j | c)\n",
    "            log p(x | c) = ∑_j log p(x_j | c)\n",
    "\n",
    "        Each feature likelihood p(x_j | c) is modeled as a univariate Gaussian\n",
    "        distribution with class-specific mean and variance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_row : array-like of shape (n_features,)\n",
    "            A single data point.\n",
    "        class_label : object\n",
    "            The class for which likelihood is being computed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The total log-likelihood log p(x | c), computed as the sum of \n",
    "            Gaussian log-densities across all features.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "            - This method operates fully in log space to ensure numerical stability.\n",
    "            - The method assumes that Gaussian means and variances for each class\n",
    "            have already been estimated and stored in:\n",
    "                - self.gaussian_means_[class_label]\n",
    "                - self.gaussian_vars_[class_label]\n",
    "\n",
    "            Algorithm\n",
    "            ---------\n",
    "            1. Initialize log-likelihood accumulator log_l = 0\n",
    "            2. For each feature j:\n",
    "                a. Retrieve class-specific mean μ_{c,j} and variance σ²_{c,j}\n",
    "                b. Compute log p(x_j | c) using a Gaussian log-pdf\n",
    "                c. Add the result to log_l\n",
    "            3. Return the accumulated log-likelihood\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0}}\n",
    "        >>> model.gaussian_vars_ = {'A': {0: 1.0}}\n",
    "        >>> model._compute_log_likelihood([0.0], 'A')\n",
    "        0.0\n",
    "\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0, 1: 1.0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1.0, 1: 1.0}}\n",
    "        >>> model._compute_log_likelihood([0.0, 1.0], 'A')\n",
    "        -1.8378770664093453\n",
    "        \"\"\"\n",
    "        log_l = 0.0\n",
    "\n",
    "        for j in range(len(x_row)):\n",
    "            mean = self.gaussian_means_[class_label][j]\n",
    "            var  = self.gaussian_vars_[class_label][j]\n",
    "            log_l += self._gaussian_log_pdf(x_row[j], mean, var)\n",
    "\n",
    "        return log_l\n",
    "\n",
    "    # Fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Gaussian Naive Bayes model using training data.\n",
    "\n",
    "        This method estimates:\n",
    "        - Class prior probabilities P(Y = c)\n",
    "        - Per-class per-feature Gaussian parameters (mean and variance)\n",
    "\n",
    "        under the Naive Bayes conditional independence assumption.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training feature matrix consisting of continuous numerical features.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Class labels corresponding to each training sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the fitted model instance.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If X contains missing values (NaN). This implementation assumes\n",
    "            all features are fully observed.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - Variance smoothing is applied to each feature variance using:\n",
    "            var_jc ← var_jc + var_smoothing * max_j Var(X_j)\n",
    "        where Var(X_j) is the variance of feature j across the entire dataset.\n",
    "        - All probability-related quantities are stored in log space.\n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        if np.isnan(X).any():\n",
    "            raise ValueError(\"GaussianNaiveBayes does not support missing values. \"\n",
    "                     \"Please remove or impute missing data before fitting.\")\n",
    "\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        feature_vars = X.var(axis=0)          # per-feature variance\n",
    "        max_var = np.max(feature_vars)        # biggest variance across features\n",
    "        epsilon = self.var_smoothing * max_var \n",
    "\n",
    "        # class priors\n",
    "        for c in self.classes_:\n",
    "            n_c = np.sum(y == c)\n",
    "            self.log_priors_[c] = np.log(n_c / n_samples)\n",
    "\n",
    "        # init Gaussian dicts\n",
    "        for c in self.classes_:\n",
    "            self.gaussian_means_[c] = {}\n",
    "            self.gaussian_vars_[c] = {}\n",
    "\n",
    "        # compute Gaussian parameters\n",
    "        for c in self.classes_:\n",
    "            Xc = X[y == c]\n",
    "\n",
    "            means = Xc.mean(axis=0)\n",
    "            vars_  = Xc.var(axis=0)\n",
    "\n",
    "            # smoothing:\n",
    "            # var += eps * global_variance\n",
    "            smoothed_vars = vars_ + epsilon\n",
    "\n",
    "            for j in range(n_features):\n",
    "                self.gaussian_means_[c][j] = means[j]\n",
    "                self.gaussian_vars_[c][j] = smoothed_vars[j]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Prediction\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute unnormalized log posterior scores log P(Y = c | X) for each sample\n",
    "        and each class using Gaussian Naive Bayes.\n",
    "\n",
    "        Using Bayes' rule, the posterior is proportional to the product of the\n",
    "        class prior and the class-conditional likelihood:\n",
    "            P(c | x) ∝ P(c) · P(x | c)\n",
    "\n",
    "        Taking the logarithm gives:\n",
    "            log P(c | x) = log P(c) + log P(x | c)\n",
    "\n",
    "        where log P(x | c) is computed under the Naive Bayes conditional\n",
    "        independence (i.i.d.) assumption as the sum of per-feature Gaussian\n",
    "        log-likelihoods.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples, where each row corresponds to a single data point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Log posterior scores (not normalized).\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        - All computations are performed in log space for numerical stability.\n",
    "        - This method assumes that the following attributes have already been\n",
    "        estimated during fitting:\n",
    "            - self.classes_\n",
    "            - self.log_priors_\n",
    "            - self.gaussian_means_\n",
    "            - self.gaussian_vars_\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Initialize a matrix log_probs of shape (n_samples, n_classes)\n",
    "        2. For each sample i:\n",
    "            a. Extract the sample x_row = X[i]\n",
    "            b. For each class c:\n",
    "                i.   Compute log-likelihood log P(x | c) using _compute_log_likelihood\n",
    "                ii.  Add the class log prior log P(c)\n",
    "                iii. Store the result in log_probs[i, c]\n",
    "        3. Return the log_probs matrix            \n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0.0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1.0}}\n",
    "        >>> model.predict_log_proba([[0]])\n",
    "        array([[0.]])\n",
    "\n",
    "        >>> model.classes_ = np.array(['A', 'B'])\n",
    "        >>> model.log_priors_ = {'A': np.log(0.5), 'B': np.log(0.5)}\n",
    "        >>> model.gaussian_means_ = {\n",
    "        ...     'A': {0: 0.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.gaussian_vars_ = {\n",
    "        ...     'A': {0: 1.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.predict_log_proba([[0.0], [1.0]])\n",
    "        array([[-0.9189..., -1.4189...],\n",
    "            [-1.4189..., -0.9189...]])\n",
    "            \n",
    "        \"\"\"\n",
    "        X = np.asarray(X, float)\n",
    "        n_samples = X.shape[0]\n",
    "        log_probs = np.zeros((n_samples, len(self.classes_)))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x_row = X[i]\n",
    "            for k, c in enumerate(self.classes_):\n",
    "                log_l = self._compute_log_likelihood(x_row, c)\n",
    "                log_probs[i, k] = self.log_priors_[c] + log_l\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute normalized posterior probabilities P(Y = c | X) for each sample\n",
    "        using the Gaussian Naive Bayes model.\n",
    "\n",
    "        This method converts unnormalized log posterior scores obtained from\n",
    "        `predict_log_proba` into valid probability distributions by applying\n",
    "        the softmax function. To ensure numerical stability, the log-sum-exp\n",
    "        trick is used by shifting log probabilities before exponentiation.\n",
    "\n",
    "        Specifically, for each sample x:\n",
    "            P(c | x) = exp(log P(c | x)) / sum_k exp(log P(k | x))\n",
    "        where log P(c | x) = log P(c) + log P(x | c).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples, n_classes)\n",
    "            Normalized probabilities. Each row sums to 1.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - This method relies on `predict_log_proba` to compute unnormalized\n",
    "        log posterior scores.\n",
    "        - For numerical stability, the maximum log posterior value is subtracted\n",
    "        from each row before exponentiation:\n",
    "            shifted = log_probs - max(log_probs)\n",
    "\n",
    "        This does not change the resulting probabilities because softmax\n",
    "        is invariant to constant shifts.\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Compute unnormalized log posterior scores using predict_log_proba(X)\n",
    "        2. For each sample, subtract the maximum log score (numerical stability)\n",
    "        3. Exponentiate the shifted log scores\n",
    "        4. Normalize by dividing by the row-wise sum\n",
    "        5. Return normalized probabilities\n",
    "\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1}}\n",
    "        >>> model.predict_proba([[0]])\n",
    "        array([[1.]])\n",
    "\n",
    "        >>> model.classes_ = np.array(['A', 'B'])\n",
    "        >>> model.log_priors_ = {'A': np.log(0.5), 'B': np.log(0.5)}\n",
    "        >>> model.gaussian_means_ = {\n",
    "        ...     'A': {0: 0.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.gaussian_vars_ = {\n",
    "        ...     'A': {0: 1.0},\n",
    "        ...     'B': {0: 1.0}\n",
    "        ... }\n",
    "        >>> model.predict_proba([[0.0], [1.0]])\n",
    "        array([[0.6224..., 0.3775...],\n",
    "            [0.3775..., 0.6224...]])\n",
    "        \"\"\"\n",
    "        log_probs = self.predict_log_proba(X)\n",
    "        max_log = np.max(log_probs, axis=1, keepdims=True)\n",
    "        shifted = log_probs - max_log\n",
    "\n",
    "        probs = np.exp(shifted)\n",
    "        probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the most likely class label for each sample.\n",
    "\n",
    "        This method first computes normalized posterior probabilities using\n",
    "        `predict_proba`, and then selects the class with the highest probability\n",
    "        for each sample. Formally, the predicted class is:\n",
    "            y_hat = argmax_c P(Y = c | x)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "            Predicted class labels corresponding to the maximum posterior\n",
    "            probability for each sample.\n",
    "\n",
    "        Algorithm\n",
    "        ---------\n",
    "        1. Compute normalized posterior probabilities using predict_proba(X)\n",
    "        2. Find the index of the maximum probability for each sample\n",
    "        3. Map indices to class labels using self.classes_\n",
    "        4. Return predicted class labels\n",
    "\n",
    "        Example\n",
    "        -------\n",
    "        >>> X = np.array([[0], [2]])\n",
    "        >>> model = GaussianNaiveBayes()\n",
    "        >>> model.classes_ = np.array(['A'])\n",
    "        >>> model.log_priors_ = {'A': 0.0}\n",
    "        >>> model.gaussian_means_ = {'A': {0: 0}}\n",
    "        >>> model.gaussian_vars_  = {'A': {0: 1}}\n",
    "        >>> model.predict(X)\n",
    "        array(['A', 'A'], dtype='<U1')\n",
    "        \"\"\"\n",
    "        probs = self.predict_proba(X)\n",
    "        class_indices = np.argmax(probs, axis=1)\n",
    "        return self.classes_[class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218cc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data and basic preprocessing\n",
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "df[\"diagnosis\"] = df[\"diagnosis\"].map({\"M\": 1, \"B\": 0})\n",
    "df = df.dropna()\n",
    "\n",
    "X = df.drop(columns=[\"id\", \"diagnosis\"]).values.astype(float)\n",
    "y = df[\"diagnosis\"].values\n",
    "\n",
    "# Train–test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Our own Gaussian NB\n",
    "my_nb = GaussianNaiveBayes()\n",
    "my_nb.fit(X_train, y_train)\n",
    "y_pred_my = my_nb.predict(X_test)\n",
    "proba_my = my_nb.predict_proba(X_test)\n",
    "\n",
    "# sklearn GaussianNB (reference)\n",
    "sk_nb = GaussianNB(var_smoothing=1e-9)\n",
    "sk_nb.fit(X_train, y_train)\n",
    "y_pred_sk = sk_nb.predict(X_test)\n",
    "proba_sk = sk_nb.predict_proba(X_test)\n",
    "\n",
    "print(\"My accuracy:\", accuracy_score(y_test, y_pred_my))\n",
    "print(\"sklearn accuracy:\", accuracy_score(y_test, y_pred_sk))\n",
    "\n",
    "# 1) Compare predicted labels\n",
    "same_labels = np.array_equal(y_pred_my, y_pred_sk)\n",
    "print(\"Class predictions exactly equal:\", same_labels)\n",
    "\n",
    "# 2) Compare predicted probabilities (after aligning class order)\n",
    "print(\"\\nMy classes_     :\", my_nb.classes_)\n",
    "print(\"sklearn classes_:\", sk_nb.classes_)\n",
    "\n",
    "order_my = np.argsort(my_nb.classes_)\n",
    "order_sk = np.argsort(sk_nb.classes_)\n",
    "\n",
    "proba_my_aligned = proba_my[:, order_my]\n",
    "proba_sk_aligned = proba_sk[:, order_sk]\n",
    "\n",
    "# Check if probabilities match up to floating-point tolerance\n",
    "close_probs = np.allclose(\n",
    "    proba_my_aligned,\n",
    "    proba_sk_aligned,\n",
    "    rtol=1e-8,\n",
    "    atol=1e-10,\n",
    ")\n",
    "print(\"Probabilities numerically equal (up to tolerance):\", close_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
